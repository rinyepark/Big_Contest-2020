{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51q17oX0eCcD"
   },
   "source": [
    "## 1. 제공된 데이터 합치기\n",
    "**<p>2016 ~ 2020년 9월 26일까지의 팀타자/팀투수 파일 통합 </p>**\n",
    "\n",
    "주최 측에서 제공한 데이터(2016 ~ 2020.07)와 그 이후의 2020년도 경기 데이터(2020.07 ~ 2020.09)를 수집해 기존 파일과 같은 형식으로 처리한 후 투수/타자 데이터로 통합하였다. \n",
    "- 수집한 2020년 타자 추가데이터: \"../data/2020빅콘테스트_타자_추가데이터.csv\"\n",
    "- 수집한 2020년 투수 추가데이터: \"../data/2020빅콘테스트_투수_추가데이터.csv\"  \n",
    "\n",
    "기존데이터와 변수를 통합하기 위해 크롤링한 데이터로 구할 수 없는 column들인 \"P2_WHIP_RT\"(투수), \"CB_WHIP_RT\"(투수), \"LOB\"(타자), \"P_AB_CN\"(타자), \"P_HIT_CN\"(타자) 지표는 불가피하게 탈락시켰다.  <br>\n",
    "예측 해야할 기간에 가장 중요한 영향을 미치는 정보가 예측해야 하는 경기와 가장 가까운 기간에 있는 데이터라고 판단하였기에 위와 같은 조취를 취했다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1600447143948,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "EFIYh1LXbqk9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pitcher_df_2016 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2016.csv\", encoding=\"utf-8-sig\").drop([\"P2_WHIP_RT\", \"CB_WHIP_RT\"], axis = 1)\n",
    "pitcher_df_2017 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2017.csv\", encoding=\"utf-8-sig\").drop([\"P2_WHIP_RT\", \"CB_WHIP_RT\"], axis = 1) \n",
    "pitcher_df_2018 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2018.csv\", encoding=\"utf-8-sig\").drop([\"P2_WHIP_RT\", \"CB_WHIP_RT\"], axis = 1) \n",
    "pitcher_df_2019 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2019.csv\", encoding=\"utf-8-sig\").drop([\"P2_WHIP_RT\", \"CB_WHIP_RT\"], axis = 1) \n",
    "pitcher_df_2020 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀투수_2020.csv\", encoding=\"utf-8-sig\").drop([\"P2_WHIP_RT\", \"CB_WHIP_RT\"], axis = 1) \n",
    "pitcher_new = pd.read_csv(\"../data/2020빅콘테스트_타자_추가데이터.csv\", encoding=\"utf-8-sig\") \n",
    "pitcher_df = pd.concat([pitcher_df_2016, pitcher_df_2017, pitcher_df_2018, pitcher_df_2019, pitcher_df_2020, pitcher_new])\n",
    "\n",
    "hitter_df_2016 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2016.csv\", encoding=\"utf-8-sig\").drop([\"LOB\", \"P_AB_CN\", \"P_HIT_CN\"], axis = 1) \n",
    "hitter_df_2017 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2017.csv\", encoding=\"utf-8-sig\").drop([\"LOB\", \"P_AB_CN\", \"P_HIT_CN\"], axis = 1) \n",
    "hitter_df_2018 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2018.csv\", encoding=\"utf-8-sig\").drop([\"LOB\", \"P_AB_CN\", \"P_HIT_CN\"], axis = 1) \n",
    "hitter_df_2019 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2019.csv\", encoding=\"utf-8-sig\").drop([\"LOB\", \"P_AB_CN\", \"P_HIT_CN\"], axis = 1) \n",
    "hitter_df_2020 = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀타자_2020.csv\", encoding=\"utf-8-sig\").drop([\"LOB\", \"P_AB_CN\", \"P_HIT_CN\"], axis = 1) \n",
    "hitter_new = pd.read_csv(\"../data/2020빅콘테스트_투수_추가데이터.csv\", encoding=\"utf-8-sig\") \n",
    "hitter_df = pd.concat([hitter_df_2016, hitter_df_2017, hitter_df_2018, hitter_df_2019, hitter_df_2020, hitter_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RpjO37RreMM"
   },
   "source": [
    "## 2. 데이터 전처리하기\n",
    "### 2-1. T_ID 기준 데이터 전처리\n",
    "**투수 변수와 타자 변수 분리**\n",
    "\n",
    "<p>투수 지표와 타자 지표에서 겹치는 column을 구분하기 위해서 타자 지표 앞에는 h_, 투수 지표 앞에는 p_를 붙인 후 파일을 합치는 작업을 수행했다.<br>\n",
    "식별을 쉽게 해주기 위해서 해당 경기의 진행년도 열을 YEAR라는 이름으로 추가하였다. <br> \n",
    "GAME ID와 GAME DAY, HEADER 경기 여부 등은는 팀타자, 팀투수 파일에 모두 있기 때문에 중복되지 않게 하였다. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pitcher_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-550cb517ff0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 데이터 column 순서 맞춰주기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m pitcher_df=pitcher_df[['G_ID','GDAY_DS', 'T_ID', 'VS_T_ID', 'HEADER_NO', 'TB_SC', 'CG_CK', 'WLS', 'HOLD', 'INN2', 'BF', 'PA', 'AB', 'HIT', 'H2', 'H3',\n\u001b[0m\u001b[0;32m      3\u001b[0m  'HR', 'SB', 'CS', 'SH', 'SF', 'BB', 'IB', 'HP', 'KK', 'GD', 'WP', 'BK', 'ERR', 'R', 'ER', 'P_WHIP_RT']]\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m hitter_df=hitter_df[['G_ID', 'GDAY_DS', 'T_ID', 'VS_T_ID', 'HEADER_NO', 'TB_SC', 'PA', 'AB', 'RBI', 'RUN', 'HIT', 'H2', 'H3', 'HR', 'SB',\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pitcher_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터 column 순서 맞춰주기\n",
    "pitcher_df=pitcher_df[['G_ID','GDAY_DS', 'T_ID', 'VS_T_ID', 'HEADER_NO', 'TB_SC', 'CG_CK', 'WLS', 'HOLD', 'INN2', 'BF', 'PA', 'AB', 'HIT', 'H2', 'H3',\n",
    " 'HR', 'SB', 'CS', 'SH', 'SF', 'BB', 'IB', 'HP', 'KK', 'GD', 'WP', 'BK', 'ERR', 'R', 'ER', 'P_WHIP_RT']]\n",
    "\n",
    "hitter_df=hitter_df[['G_ID', 'GDAY_DS', 'T_ID', 'VS_T_ID', 'HEADER_NO', 'TB_SC', 'PA', 'AB', 'RBI', 'RUN', 'HIT', 'H2', 'H3', 'HR', 'SB',\n",
    " 'CS', 'SH', 'SF', 'BB', 'IB', 'HP', 'KK', 'GD', 'ERR', 'P_HRA_RT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1600447144212,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "O1gNT7UArfz8",
    "outputId": "2b9334a4-c804-462f-dcc9-a7760fbc6966",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G_ID</th>\n",
       "      <th>GDAY_DS</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>VS_T_ID</th>\n",
       "      <th>WLS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>HEADER_NO</th>\n",
       "      <th>p_TB_SC</th>\n",
       "      <th>p_CG_CK</th>\n",
       "      <th>p_HOLD</th>\n",
       "      <th>...</th>\n",
       "      <th>h_CS</th>\n",
       "      <th>h_SH</th>\n",
       "      <th>h_SF</th>\n",
       "      <th>h_BB</th>\n",
       "      <th>h_IB</th>\n",
       "      <th>h_HP</th>\n",
       "      <th>h_KK</th>\n",
       "      <th>h_GD</th>\n",
       "      <th>h_ERR</th>\n",
       "      <th>h_P_HRA_RT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20160401HHLG0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>LG</td>\n",
       "      <td>HH</td>\n",
       "      <td>W</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20160401HHLG0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>HH</td>\n",
       "      <td>LG</td>\n",
       "      <td>L</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20160401HTNC0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NC</td>\n",
       "      <td>HT</td>\n",
       "      <td>W</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20160401HTNC0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>HT</td>\n",
       "      <td>NC</td>\n",
       "      <td>L</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20160401KTSK0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>SK</td>\n",
       "      <td>KT</td>\n",
       "      <td>L</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            G_ID   GDAY_DS T_ID VS_T_ID WLS  YEAR  HEADER_NO p_TB_SC  p_CG_CK  \\\n",
       "0  20160401HHLG0  20160401   LG      HH   W  2016          0       1      0.0   \n",
       "1  20160401HHLG0  20160401   HH      LG   L  2016          0       0      0.0   \n",
       "2  20160401HTNC0  20160401   NC      HT   W  2016          0       1      0.0   \n",
       "3  20160401HTNC0  20160401   HT      NC   L  2016          0       0      0.0   \n",
       "4  20160401KTSK0  20160401   SK      KT   L  2016          0       1      0.0   \n",
       "\n",
       "   p_HOLD  ...  h_CS  h_SH  h_SF  h_BB  h_IB  h_HP  h_KK  h_GD  h_ERR  \\\n",
       "0     0.0  ...   1.0   1.0   0.0   4.0   0.0   0.0  11.0   0.0    0.0   \n",
       "1     0.0  ...   0.0   3.0   0.0   3.0   0.0   0.0  10.0   1.0    2.0   \n",
       "2     0.0  ...   0.0   1.0   0.0   5.0   0.0   0.0   9.0   1.0    1.0   \n",
       "3     0.0  ...   0.0   0.0   0.0   3.0   0.0   1.0  10.0   1.0    0.0   \n",
       "4     0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   7.0   1.0    1.0   \n",
       "\n",
       "   h_P_HRA_RT  \n",
       "0    0.333333  \n",
       "1    0.200000  \n",
       "2    0.142857  \n",
       "3    0.100000  \n",
       "4    0.375000  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 투수 데이터 전처리\n",
    "pitcher_df[\"TB_SC\"][pitcher_df[\"TB_SC\"] == \"B\"] = 1\n",
    "pitcher_df[\"TB_SC\"][pitcher_df[\"TB_SC\"] == \"T\"] = 0\n",
    "# pitcher_df[\"WLS\"][pitcher_df[\"WLS\"] == \"W\"] = 1\n",
    "# pitcher_df[\"WLS\"][pitcher_df[\"WLS\"] == \"L\"] = 0\n",
    "# pitcher_df[\"WLS\"][pitcher_df[\"WLS\"] == \"D\"] = -1\n",
    "p_columns = list(pitcher_df.columns)\n",
    "front_lst = p_columns[:5]\n",
    "back_lst = p_columns[5:]\n",
    "back_lst = [\"p_\" + column for column in back_lst]\n",
    "p_columns = front_lst + back_lst\n",
    "pitcher_df.columns = p_columns\n",
    "\n",
    "# 타자 데이터 전처리\n",
    "hitter_df.drop([\"G_ID\", \"GDAY_DS\", \"T_ID\", \"VS_T_ID\", \"TB_SC\", \"HEADER_NO\"], axis=1, inplace=True)\n",
    "h_columns = list(hitter_df.columns)\n",
    "h_columns = [\"h_\" + column for column in h_columns]\n",
    "hitter_df.columns = h_columns\n",
    "df = pd.concat([pitcher_df, hitter_df], axis=1)\n",
    "df.rename(columns={\"p_WLS\":\"WLS\"}, inplace=True)\n",
    "\n",
    "# YEAR 데이터 추가\n",
    "dates = df[\"GDAY_DS\"].to_list()\n",
    "year = [\"\".join(list(str(date))[:4]) for date in dates ]\n",
    "df[\"YEAR\"] = year\n",
    "\n",
    "# 인덱스 순서 변경\n",
    "df_columns = list(df.columns)\n",
    "df_columns_reindex = df_columns[:4] + [df_columns[7]] + [df_columns[-1]] + df_columns[4:7] + df_columns[8:-1]\n",
    "df = df[df_columns_reindex] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZ1xUnIeOE5M"
   },
   "source": [
    "### 2-2. VS_T_ID 기준 데이터 전처리\n",
    "**상대팀의 경기력을 나타내는 변수 추가**  \n",
    "\n",
    "<p> T_ID 기준으로 경기 경기의 결과(방어율, 타율, 승률)가 상대방의 경기력에 영향을 받는다고 판단하여 VS_T_ID(상대편)의 경기 정보를 추가했다. 상대방 또한 같은 이름의 columns를 가지고 있기 때문에 앞에 \"vs_\"를 붙여서 각 줄에 추가하였다.  \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1093,
     "status": "ok",
     "timestamp": 1600447144214,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "Cpq-WRPnOB2L"
   },
   "outputs": [],
   "source": [
    "df_vs = df.copy()\n",
    "df_vs.drop([\"WLS\", \"HEADER_NO\"],axis=1, inplace=True)\n",
    "\n",
    "merge_col = list(df.columns)[:4] + [list(df.columns)[5]]\n",
    "df_vs_col = list(df.columns)[7:]\n",
    "vs_rename_col_dict = dict()\n",
    "\n",
    "for i in df_vs_col:\n",
    "    vs_rename_col_dict[i] = 'vs_'+i\n",
    "\n",
    "vs_rename_col_dict[\"T_ID\"] = 'VS_T_ID'\n",
    "vs_rename_col_dict[\"VS_T_ID\"] = \"T_ID\"\n",
    "\n",
    "df_vs = df_vs.rename(columns = vs_rename_col_dict)\n",
    "\n",
    "df = pd.merge(df, df_vs, how = 'left', left_on = merge_col, right_on = merge_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPcAKI6hKoh1"
   },
   "source": [
    "## 3. 팀기준 24경기씩 그룹핑 및 data augmentation 진행\n",
    "**- 팀별 후반 경기(약 24경기)의 타겟값을 바로 이전 24경기에서의 경기력을 이용해 예측**  \n",
    "\n",
    "**- 그룹핑으로 인한 데이터 감소로 augmentation을 진행해 학습 데이터 부족 문제 해결**\n",
    "\n",
    "\n",
    "##### <데이터 전처리 결정 과정>\n",
    "- 1) 주어진 주제: 정규시즌 <strong>잔여 경기</strong>에 대한 각 팀별 승률, 타율, 방어율 예측\n",
    "- 2) 팀별로 값을 예측해야 함 -> 한 시즌 동안 각 구단마다 총 144경기를 진행\n",
    "- 3) 대회 종료일(9.29)기준 팀별로 144경기 중 후반부 24경기가 잔여경기로 포함될 것이라 판단(+ 경기취소 고려)  \n",
    "   - (제공된 데이터에 대한 전처리)\n",
    "\n",
    "|연도|팀별 총 경기 수 |전처리방법|\n",
    "|:-:|:-:|:-:|\n",
    "|2016 ~ 2019|144|전반 120경기를 24경기씩 묶어(총 5그룹) 모델을 학습시킨 후 후반 24경기 타겟값 예측|\n",
    "|2020|약 64|40경기, 24경기로 구분 후 전반 40경기를 24경기씩 묶어(약 1그룹) 모델을 학습시킨 후 후반 24경기 타겟값 예측|\n",
    "\n",
    "- 4) 발생한 문제점: <strong>충분한 데이터 확보 불가능</strong>\n",
    "  - ex) 한 시즌 기준으로 최대 60 row(각 팀당 6그룹 * 10구단)만 생성가능\n",
    "  - 데이터가 충분하지 않은 상태에서 모델을 학습시키면 모델이 일반화되기 어려움(오버피팅/언더피팅의 가능성이 모두 존재)\n",
    "\n",
    "- 5) 해결방안: <strong>Data Augmentation</strong>\n",
    "  - Augmentation: 모델을 충분히 훈련하는 데 필요한 데이터를 확보하는 기법\n",
    "  - 24경기씩 묶기 때문에 그룹핑하는 시작점에 따라 상대팀 구성비율 및 변수값이 변하므로 다른 데이터셋으로 이용할 수 있다고 판단\n",
    "  - ex)   \n",
    "    Case1: train = [1 2 ... 23 24] -> predict = [25 26 ... 47 48]  \n",
    "    Case2: train = [2 3 ... 24 25] -> predict = [26 27 ... 48 49]  \n",
    "    <strong>※ 데이터를 늘릴 때 주의할 점</strong>: train/test 구분 없이 모든 데이터에 대해 augmentation을 진행하면 예측해야 하는 값(test)과 유사한 값들이 모델에 학습되어 오버피팅이 발생할 수 있음. 따라서 모델의 일반화되기 위해 데이터를 늘리기 전, train과 test를 미리 구분한 후 train에 대해서만 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0LDdMaQQLlE"
   },
   "source": [
    "### 3-1. column 분류\n",
    "**size와 sum 그룹계산을 진행하기 위해 column 분류**\n",
    "\n",
    "24경기씩 묶어서 변수를 계산하기 위해 그룹함수(sum, size)를 적용한다. 이때, 승률계산 시 24그룹 내 각 경기에서 'WLS'변수 각각의 총 개수를 구해야 하므로 size를 적용해야하고 나머지 변수에는 sum을 적용해야 한다. 이를 나누어 계산하기 위해 각 그룹함수에 맞는 column을 분류했다.\n",
    "- size_df: 승패\n",
    "- sum_df: 나머지 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1600447144215,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "7b76jl0aKswT"
   },
   "outputs": [],
   "source": [
    "data_col = list(df.columns)\n",
    "team = list(df.T_ID.unique())\n",
    "\n",
    "default_col = ['G_ID', 'GDAY_DS', 'T_ID', 'VS_T_ID', \"YEAR\"]\n",
    "size_col = ['WLS']\n",
    "sum_col = list(df.columns)[6:]\n",
    "\n",
    "tmp = set(default_col)|set(size_col)|set(sum_col)\n",
    "size_df = df[default_col + size_col]\n",
    "sum_df = df[default_col + sum_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0LDdMaQQLlE"
   },
   "source": [
    "### 3-2. Size & Sum\n",
    "**24그룹 내 변수 값을 계산하기 위해 필요한 함수 정의**\n",
    "\n",
    "- make_pct: df와 ver(VS_T_ID고려 여부)를 입력받아 승률을 개수를 계산함\n",
    "- make_sum: df와 ver(VS_T_ID고려 여부, 홈팀 계산 적용여부)를 입력받아 합계를 계산함\n",
    "- caculate_df: df, valid_year(validation에 사용될 연도), key(몇 경기씩 묶을 것인지), sc(크기, 합계, vs적용합계 여부), ver(누적/비누적 여부)를 입력받아 key경기씩 묶어 계산한 후 augmentation을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver='t': VS_T_ID고려안함 & ver='vs': VS_T_ID고려\n",
    "def make_pct(df, ver='t'):\n",
    "    col1 = [\"T_ID\",\"WLS\",\"YEAR\"]\n",
    "    col2 = [\"T_ID\",\"YEAR\"]\n",
    "    if ver=='vs':\n",
    "        col1.append(\"VS_T_ID\")\n",
    "        col2.append(\"VS_T_ID\")\n",
    "        \n",
    "    tmp = pd.DataFrame({\"COUNT\":df.groupby(col1).size()}).pivot_table(\"COUNT\",col2,\"WLS\").reset_index().sort_values([\"YEAR\",\"T_ID\"]).reset_index(drop=True)\n",
    "    tmp.fillna(0,inplace= True)\n",
    "\n",
    "    if \"L\" not in list(tmp.columns):\n",
    "        tmp[\"L\"] = 0\n",
    "\n",
    "    if \"W\" not in list(tmp.columns):\n",
    "        tmp[\"W\"] = 0\n",
    "    \n",
    "    tmp[\"PCT\"] = tmp['W']/(tmp['W']+tmp['L'])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102380,
     "status": "ok",
     "timestamp": 1600447245536,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "jZslVn2XZhPA"
   },
   "outputs": [],
   "source": [
    "def make_sum(df, ver='t'):\n",
    "    col = [\"T_ID\",\"YEAR\"]\n",
    "    if ver == 'vs':\n",
    "        col.append(\"VS_T_ID\")\n",
    "    if ver == 'home':\n",
    "        col.remove('YEAR')\n",
    "    \n",
    "    tmp = df.groupby(col).sum().reset_index()\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102376,
     "status": "ok",
     "timestamp": 1600447245527,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "wFaMCSO4QOQf",
    "outputId": "0db1e738-cb74-436f-f630-2c9f69f7611c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ver='c': 누적데이터 & ver='o':순수데이터\n",
    "\n",
    "def calculate_df(df, valid_year, key, sc='sum', ver='o'):\n",
    "\n",
    "    df_cal = pd.DataFrame([])\n",
    "\n",
    "    for y in [2016,2017,2018,2019,2020]:\n",
    "        print(\"===\",y)\n",
    "        y_tmp = df[df[\"YEAR\"]==str(y)]\n",
    "        for t in team:\n",
    "            print(t)\n",
    "            \n",
    "            ttmp_df = y_tmp[y_tmp[\"T_ID\"]==t]\n",
    "            tmp_size = len(ttmp_df)\n",
    "\n",
    "            #validset\n",
    "            if y in valid_year:\n",
    "                if ver == 'c':\n",
    "                    test_tmp_df = ttmp_df.iloc[:tmp_size-key,:]\n",
    "                else:\n",
    "                    test_tmp_df = ttmp_df.iloc[tmp_size-key:,:]\n",
    "                    \n",
    "                if sc == 'size':\n",
    "                    tmp2 = make_pct(test_tmp_df)\n",
    "                elif sc == 'sum_ae':\n",
    "                    tmp2 = make_sum(test_tmp_df,'vs')\n",
    "                else:\n",
    "                    tmp2 = make_sum(test_tmp_df)\n",
    "                tmp2['IDX'] = 777\n",
    "                tmp2['MERGE_IDX'] = 0\n",
    "\n",
    "                df_cal = pd.concat([df_cal, tmp2], axis = 0)\n",
    "\n",
    "            # trainset\n",
    "            if y in valid_year: # valid_data인 경우->120개 이용\n",
    "                tmp_df = ttmp_df.iloc[:tmp_size-key,:]\n",
    "                t_size = len(tmp_df)\n",
    "            else: # train_data인 경우->전체 데이터이용\n",
    "                tmp_df = ttmp_df.copy()\n",
    "                t_size = len(tmp_df)\n",
    "            \n",
    "            idx = 0\n",
    "            m_idx = 1\n",
    "            while idx < key:\n",
    "                k_idx = 1\n",
    "                for k in range((t_size-idx)//key):\n",
    "                    \n",
    "                    if ver == 'c':\n",
    "                        tmp1 = tmp_df.iloc[:key*(k+1)+idx,:]\n",
    "                    else:\n",
    "                        tmp1 = tmp_df.iloc[key*k+idx:key*(k+1)+idx,:]\n",
    "                    \n",
    "                    if sc == 'size':\n",
    "                        tmp2 = make_pct(tmp1)\n",
    "                    elif sc == 'sum_ae':\n",
    "                        tmp2 = make_sum(tmp1,'vs')\n",
    "                    else:\n",
    "                        tmp2 = make_sum(tmp1)\n",
    "                        \n",
    "                    tmp2['IDX'] = k_idx \n",
    "                    tmp2['MERGE_IDX'] = m_idx\n",
    "\n",
    "                    k_idx += 1\n",
    "                    m_idx += 1\n",
    "\n",
    "                    df_cal = pd.concat([df_cal, tmp2], axis = 0)\n",
    "                idx += 1\n",
    "                \n",
    "    if sc == 'size':\n",
    "        df_cal.drop([\"W\",\"D\",\"L\"], axis = 1, inplace = True)\n",
    "    \n",
    "    df_cal = df_cal.sort_values(by=[\"YEAR\",\"T_ID\",\"MERGE_IDX\"]).reset_index(drop = True)\n",
    "    return df_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 추가 데이터 생성\n",
    "**HOME비율/상대팀 구성비율, 누적 상대타율/방어율 계산**  \n",
    "\n",
    "PCT/AVG/ERA의 입력 변수에 각 그룹(24경기) 내에서 T_ID팀이 HOME팀이었던 경우의 합(ISHOME)과 상대팀 구성비율(c_00)을 추가하기 위해 전처리에 필요한 변수만 추출한 임시 테이블을 생성했다.  \n",
    "<br>\n",
    "상대 팀(VS_T_ID)에 대한 누적 상대 타율/방어율을 계산해 AVG/ERA 입력변수에 추가하기 위해 전처리에 필요한 변수만 추출한 임시 테이블을 생성했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. HOME팀 및 상대팀 구성 비율  \n",
    "\n",
    "T_ID를 기준으로 홈팀이었으면 1, 아니면 0의 값을 가지는 'ISHOME'변수를 추가한 후, 필요한 변수가 담긴 테이블을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df, pd.get_dummies(df.VS_T_ID)], axis = 1)\n",
    "\n",
    "for i in range(len(df_new)):\n",
    "    if df_new.loc[i,'T_ID'] == df_new.loc[i,\"G_ID\"][10:12]:\n",
    "        df_new.loc[i,'ISHOME'] = 1\n",
    "    else:\n",
    "        df_new.loc[i,'ISHOME'] = 0\n",
    "        \n",
    "df_new[\"ISHOME\"] = df_new[\"ISHOME\"].astype(np.int64)\n",
    "\n",
    "home_n_vs_df = df_new[[\"G_ID\",\"YEAR\",\"T_ID\",\"VS_T_ID\",\"ISHOME\"]+team]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 누적 상대타율/방어율  \n",
    "\n",
    "누적 타율, 방어율 계산을 위한 변수와 기본 정보가 담긴 테이블을 추출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_col = ['G_ID','T_ID', 'VS_T_ID',\"YEAR\"]\n",
    "ae_col = ['p_INN2', 'p_ER', 'h_HIT','h_AB']\n",
    "ae_df = df[def_col + ae_col]\n",
    "\n",
    "ae_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. 데이터 생성  \n",
    "\n",
    "홈팀합계/상대팀 구성비율 및 누적 타율/방어율을 계산해 데이터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n"
     ]
    }
   ],
   "source": [
    "year = [2016,2017,2018,2019]\n",
    "\n",
    "Home_VS_lst = []\n",
    "\n",
    "cteam_AVG_lst = []\n",
    "cteam_ERA_lst = []\n",
    "\n",
    "for y in year:\n",
    "    valid_y = year[:]\n",
    "    valid_y.remove(y)\n",
    "    \n",
    "    # home and vs_count\n",
    "    cal_h_vs_df = calculate_df(home_n_vs_df, valid_y, 24, sc='sum', ver='o')\n",
    "    cal_h_vs_df_c = cal_h_vs_df.add_prefix('c_').iloc[:,3:13]\n",
    "    cal_h_vs_df_c = cal_h_vs_df_c/24\n",
    "    cal_hvs_df = pd.concat([cal_h_vs_df.iloc[:,[0,1,2,13,14]],cal_h_vs_df_c],axis=1)\n",
    "    \n",
    "    Home_VS_lst.append(cal_hvs_df)\n",
    "\n",
    "    # cumulate vs era & vs avg\n",
    "    cal_ae_df = calculate_df(ae_df,valid_y ,24, sc='sum_ae', ver='c')\n",
    "\n",
    "    cal_ae_df[\"ERA\"] = (9 * cal_ae_df[\"p_ER\"]) / (cal_ae_df[\"p_INN2\"] / 3)\n",
    "    cal_ae_df[\"AVG\"] = (cal_ae_df[\"h_HIT\"] / cal_ae_df[\"h_AB\"])\n",
    "\n",
    "    cul_t_avg_df = cal_ae_df.drop([\"ERA\"]+ae_col,axis=1)\n",
    "    cul_t_era_df = cal_ae_df.drop([\"AVG\"]+ae_col,axis=1)\n",
    "    \n",
    "    cul_avg_df = cul_t_avg_df.pivot_table('AVG', ['T_ID', 'YEAR', 'IDX', 'MERGE_IDX'], 'VS_T_ID').reset_index().sort_values(by =[\"YEAR\",\"T_ID\",\"MERGE_IDX\"]).reset_index(drop = True).fillna(0)\n",
    "    cul_era_df = cul_t_era_df.pivot_table('ERA', ['T_ID', 'YEAR', 'IDX', 'MERGE_IDX'], 'VS_T_ID').reset_index().sort_values(by =[\"YEAR\",\"T_ID\",\"MERGE_IDX\"]).reset_index(drop = True).fillna(0)\n",
    "\n",
    "    ca = cul_avg_df.iloc[:,-10:].add_prefix('a_')\n",
    "    ce = cul_era_df.iloc[:,-10:].add_prefix('e_')\n",
    "    \n",
    "    cteam_AVG_lst.append(ca)\n",
    "    cteam_ERA_lst.append(ce)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 합치면서 세이버매트릭스 변환  \n",
    "**기본 변수를 세이버매트릭스를 이용해 변환하여 모델 입력값 생성**\n",
    "\n",
    "주어진 기본 변수를 활용하기에는 차원이 커지고 유의미하지 않은 변수가 섞여 모델이 제대로 학습되지 않을 수 있으므로, 세이버 매트릭스를 적용해 보다 유용한 변수를 계산하는 과정을 거쳤다.\n",
    "\n",
    "- <strong>합계한 column으로 세이버매트릭스 적용하는 이유</strong> : 각 경기마다 세이버매트릭스로 변환해 24경기로 평균을 구한 값과, 24경기 동안 나온 결과들을 합쳐 세이버매트릭스로 변환한 값의 오차가 발생하여 합계 후 세이버매트릭스 변환을 적용하게 됨\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1. 세이버매트릭스 변환 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 부분\n",
    "# T_ID, YEAR, PCT, IDX, MERGE_IDX\n",
    "\n",
    "# T_ID 팀\n",
    "# p_CG_CK, p_HOLD, p_INN2, p_BF, p_PA, p_AB, p_HIT, p_H2, p_H3\n",
    "# p_HR, p_SB, p_CS, p_SH, p_SF, p_BB, p_IB, p_HP, p_KK, p_GD, p_WP, p_BK, p_ERR, p_R, p_ER, p_P_WHIP_RT, p_P2_WHIP_RT\n",
    "# p_CB_WHIP_RT, h_PA, h_AB, h_RBI, h_RUN, h_HIT, h_H2, h_H3, h_HR, h_SB, h_CS, h_SH, h_SF, h_BB, h_IB, h_HP, h_KK\n",
    "# h_GD, h_ERR, h_LOB, h_P_HRA_RT, h_P_AB_CN, h_P_HIT_CN\n",
    "\n",
    "# vs_T_ID 팀 (vs_만 붙이면 같음)\n",
    "\n",
    "'''\n",
    "@ Augment를 세이버매트릭스로 변환하기 전 주의사항\n",
    "1. P_WHIP_RT, P2_WHIP_RT, CB_WHIP_RT, P_HRA_RT를 그대로 누적했는데 그대로 누적해서\n",
    "사용하면 안되는 값이다.\n",
    "- 값이 너무 커서 일단 24로 나눠서 경기 당 평균값으로 만들었다\n",
    "- RC도 값이 커서 24로 나눠서 경기 당 평균값으로 만들었다.\n",
    "\n",
    "2.변환 이후 정규화\n",
    "\n",
    "3.이닝 수, 득점, 타수, 실점 기록 추가가 필요할까..? (승률에서는 일단 제외)\n",
    "'''\n",
    "\n",
    "\n",
    "def convert_attribute(df, ver ='tv'):\n",
    "    #column\n",
    "\n",
    "    new_index = [\"T_ID\", \"YEAR\", \"PCT\", \"IDX\", \"MERGE_IDX\",\n",
    "             \"WHIP\", \"LOB\", \"ERA\", \"FIP\", \"H_9\", \"K_9\", \"BB_9\", \"HR_9\",\n",
    "             \"AVG\", \"oAVG\", \"SLG\", \"oSLG\", \"OBP\", \"oOBP\", \"P_WHIP_RT\",\n",
    "             \"OPS\", \"oOPS\", \"RC\", \"GPA\", \"SECA\", \"TA\", \"ISO\", \"oISO\", \"wOBA\", \"P_HRA_RT\", \"DER\",\n",
    "             \"vs_WHIP\", \"vs_LOB\", \"vs_ERA\", \"vs_FIP\", \"vs_H_9\", \"vs_K_9\", \"vs_BB_9\", \"vs_HR_9\",\n",
    "             \"vs_AVG\", \"vs_oAVG\", \"vs_SLG\", \"vs_oSLG\", \"vs_OBP\", \"vs_oOBP\", \"vs_P_WHIP_RT\",\n",
    "             \"vs_OPS\", \"vs_oOPS\", \"vs_RC\", \"vs_GPA\", \"vs_SECA\", \"vs_TA\", \"vs_ISO\", \"vs_oISO\", \"vs_wOBA\", \"vs_P_HRA_RT\", \"vs_DER\"]\n",
    "    \n",
    "    if ver == 'test':\n",
    "        new_index.remove(\"IDX\")\n",
    "        new_index.remove(\"MERGE_IDX\")\n",
    "        \n",
    "    \n",
    "    #시작\n",
    "    new_df = pd.DataFrame(columns = new_index)\n",
    "    \n",
    "    # 공통 부분T_ID, YEAR, PCT, IDX, MERGE_IDX\n",
    "    new_df[\"T_ID\"] = df[\"T_ID\"]\n",
    "    new_df[\"YEAR\"] = df[\"YEAR\"]\n",
    "    new_df[\"PCT\"] = df[\"PCT\"]\n",
    "    \n",
    "    if ver != 'test':\n",
    "        new_df[\"IDX\"] = df[\"IDX\"]\n",
    "        new_df[\"MERGE_IDX\"] = df[\"MERGE_IDX\"]\n",
    "    \n",
    "    \n",
    "    # 자주 사용되는 변환 값\n",
    "    p_H1 = (df[\"p_HIT\"] - df[\"p_H2\"] - df[\"p_H3\"] - df[\"p_HR\"]) #피 1루타\n",
    "    h_H1 = (df[\"h_HIT\"] - df[\"h_H2\"] - df[\"h_H3\"] - df[\"h_HR\"]) #1 루타\n",
    "    RC_A = (df[\"h_HIT\"] + df[\"h_BB\"] - df[\"h_CS\"] + df[\"h_HP\"] - df[\"h_GD\"]) #출루능력\n",
    "    RC_B = 0.24 * (df[\"h_BB\"] - df[\"h_IB\"] + df[\"h_HP\"]) + (0.62 * df[\"h_SB\"]) + 0.5 * (df[\"h_SH\"] + df[\"h_SF\"]) - 0.03 * df[\"h_KK\"] #진루능력\n",
    "    RC_C = (df[\"h_AB\"] + df[\"h_BB\"] + df[\"h_HP\"] + df[\"h_SH\"] + df[\"h_SF\"]) #주어진 기회\n",
    "    \n",
    "    vs_p_H1 = (df[\"vs_p_HIT\"] - df[\"vs_p_H2\"] - df[\"vs_p_H3\"] - df[\"vs_p_HR\"]) #피 1루타\n",
    "    vs_h_H1 = (df[\"vs_h_HIT\"] - df[\"vs_h_H2\"] - df[\"vs_h_H3\"] - df[\"vs_h_HR\"]) #1 루타\n",
    "    vs_RC_A = (df[\"vs_h_HIT\"] + df[\"vs_h_BB\"] - df[\"vs_h_CS\"] + df[\"vs_h_HP\"] - df[\"vs_h_GD\"]) #출루능력\n",
    "    vs_RC_B = 0.24 * (df[\"vs_h_BB\"] - df[\"vs_h_IB\"] + df[\"vs_h_HP\"]) + (0.62 * df[\"vs_h_SB\"]) + 0.5 * (df[\"vs_h_SH\"] + df[\"vs_h_SF\"]) - 0.03 * df[\"vs_h_KK\"] #진루능력\n",
    "    vs_RC_C = (df[\"vs_h_AB\"] + df[\"vs_h_BB\"] + df[\"vs_h_HP\"] + df[\"vs_h_SH\"] + df[\"vs_h_SF\"]) #주어진 기회\n",
    "    #----------------------- T_ID\n",
    "    new_df[\"WHIP\"] = (df[\"p_HIT\"] + df[\"p_BB\"]) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"LOB\"] = (df[\"p_HIT\"] + df[\"p_BB\"] + df[\"p_HP\"] - df[\"p_R\"]) / (df[\"p_HIT\"] + df[\"p_BB\"] + df[\"p_HP\"] - (1.4 * df[\"p_HR\"]))\n",
    "    new_df[\"ERA\"] = (9 * df[\"p_ER\"]) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"FIP\"] = ((13 * df[\"p_HR\"]) + (3 * (df[\"p_BB\"] + df[\"p_HP\"] - df[\"p_IB\"])) - (2 * df[\"p_KK\"])) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"H_9\"] = (9 * df[\"p_HIT\"]) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"K_9\"] = (9 * df[\"p_KK\"]) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"BB_9\"] = (9 * df[\"p_BB\"]) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"HR_9\"] = (9 * df[\"p_HR\"]) / (df[\"p_INN2\"] / 3)\n",
    "    new_df[\"AVG\"] = (df[\"h_HIT\"] / df[\"h_AB\"])\n",
    "    new_df[\"oAVG\"] = (df[\"p_HIT\"] / (df[\"p_PA\"] - df[\"p_BB\"] - df[\"p_HP\"] - df[\"p_SF\"] - df[\"p_SB\"]))\n",
    "    new_df[\"SLG\"] = (h_H1 + df[\"h_H2\"] * 2 + df[\"h_H3\"] * 3 + df[\"h_HR\"] * 4) / (df[\"h_AB\"])\n",
    "    new_df[\"oSLG\"] = (p_H1 + (2 * df[\"p_H2\"]) + (3 * df[\"p_H3\"]) + (4 * df[\"p_HR\"])) / (df[\"p_PA\"] - df[\"p_BB\"] - df[\"p_HP\"] - df[\"p_SF\"] - df[\"p_SB\"])\n",
    "    new_df[\"OBP\"] = (df[\"h_HIT\"] + df[\"h_BB\"] + df[\"h_HP\"]) / (df[\"h_AB\"] + df[\"h_BB\"] + df[\"h_HP\"] + df[\"h_SF\"])\n",
    "    new_df[\"oOBP\"] = (df[\"p_HIT\"] + df[\"p_BB\"]) / (df[\"p_AB\"] + df[\"p_BB\"] + df[\"p_HP\"] + df[\"p_SF\"])\n",
    "    new_df[\"P_WHIP_RT\"] = (df[\"p_P_WHIP_RT\"] / 24) \n",
    "    new_df[\"OPS\"] = (new_df[\"OBP\"] + new_df[\"SLG\"])\n",
    "    new_df[\"oOPS\"] = (new_df[\"oOBP\"] + new_df[\"oSLG\"])\n",
    "    new_df[\"RC\"] = (((2.4 * RC_C + RC_A) * (3 * RC_C + RC_B)) / (9 * RC_C) - (0.9 * RC_C)) / 24\n",
    "    new_df[\"GPA\"] = ((1.8 * new_df[\"OBP\"] + new_df[\"SLG\"]) / 4)\n",
    "    new_df[\"SECA\"] = (((df[\"h_H2\"] * 2) + (df[\"h_H3\"] * 3) + (df[\"h_HR\"] * 4) + df[\"h_BB\"] + (df[\"h_SB\"] - df[\"h_CS\"])) / df[\"h_AB\"])\n",
    "    new_df[\"TA\"] = ((h_H1 + (df[\"h_H2\"] * 2) + (df[\"h_H3\"] * 3) + (df[\"h_HR\"] * 4) + df[\"h_HP\"] + df[\"h_BB\"] + df[\"h_SB\"]) / (df[\"h_AB\"] - df[\"h_HIT\"] + df[\"h_CS\"] + df[\"h_GD\"]))\n",
    "    new_df[\"ISO\"] = (new_df[\"SLG\"] - new_df[\"AVG\"])\n",
    "    new_df[\"oISO\"] = (new_df[\"oSLG\"] - new_df[\"oAVG\"])\n",
    "    new_df[\"wOBA\"] = ((0.7 * (df[\"h_BB\"] - df[\"h_IB\"]) + (0.73 * df[\"h_HP\"]) + (0.89 * h_H1) + (1.27 * df[\"h_H2\"]) + (1.61 * df[\"h_H3\"]) + (2.07 * df[\"h_HR\"]) + (0.25 * df[\"h_SB\"]) -\n",
    "                (0.5 * df[\"h_CS\"])) / (df[\"h_AB\"] - df[\"h_IB\"]))\n",
    "    new_df[\"P_HRA_RT\"] = df[\"h_P_HRA_RT\"] \n",
    "    new_df[\"DER\"] = ((df[\"p_PA\"] - df[\"p_HIT\"] - df[\"p_KK\"] - df[\"p_BB\"] - df[\"p_HP\"] - df[\"p_IB\"] - df[\"p_BK\"]) / (df[\"p_PA\"] - df[\"p_HR\"] - df[\"p_KK\"] - df[\"p_BB\"] - df[\"p_HP\"]))\n",
    "    \n",
    "    \n",
    "    #----------------------- vs_T_ID\n",
    "    new_df[\"vs_WHIP\"] = (df[\"vs_p_HIT\"] + df[\"vs_p_BB\"]) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_LOB\"] = (df[\"vs_p_HIT\"] + df[\"vs_p_BB\"] + df[\"vs_p_HP\"] - df[\"vs_p_R\"]) / (df[\"vs_p_HIT\"] + df[\"vs_p_BB\"] + df[\"vs_p_HP\"] - (1.4 * df[\"vs_p_HR\"]))\n",
    "    new_df[\"vs_ERA\"] = (9 * df[\"vs_p_ER\"]) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_FIP\"] = ((13 * df[\"vs_p_HR\"]) + (3 * (df[\"vs_p_BB\"] + df[\"vs_p_HP\"] - df[\"vs_p_IB\"])) - (2 * df[\"vs_p_KK\"])) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_H_9\"] = (9 * df[\"vs_p_HIT\"]) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_K_9\"] = (9 * df[\"vs_p_KK\"]) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_BB_9\"] = (9 * df[\"vs_p_BB\"]) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_HR_9\"] = (9 * df[\"vs_p_HR\"]) / (df[\"vs_p_INN2\"] / 3)\n",
    "    new_df[\"vs_AVG\"] = (df[\"vs_h_HIT\"] / df[\"vs_h_AB\"])\n",
    "    new_df[\"vs_oAVG\"] = (df[\"vs_p_HIT\"] / (df[\"vs_p_PA\"] - df[\"vs_p_BB\"] - df[\"vs_p_HP\"] - df[\"vs_p_SF\"] - df[\"vs_p_SB\"]))\n",
    "    new_df[\"vs_SLG\"] = (vs_h_H1 + df[\"vs_h_H2\"] * 2 + df[\"vs_h_H3\"] * 3 + df[\"vs_h_HR\"] * 4) / (df[\"vs_h_AB\"])\n",
    "    new_df[\"vs_oSLG\"] = (vs_p_H1 + (2 * df[\"vs_p_H2\"]) + (3 * df[\"vs_p_H3\"]) + (4 * df[\"vs_p_HR\"])) / (df[\"vs_p_PA\"] - df[\"vs_p_BB\"] - df[\"vs_p_HP\"] - df[\"vs_p_SF\"] - df[\"vs_p_SB\"])\n",
    "    new_df[\"vs_OBP\"] = (df[\"vs_h_HIT\"] + df[\"vs_h_BB\"] + df[\"vs_h_HP\"]) / (df[\"vs_h_AB\"] + df[\"vs_h_BB\"] + df[\"vs_h_HP\"] + df[\"vs_h_SF\"])\n",
    "    new_df[\"vs_oOBP\"] = (df[\"vs_p_HIT\"] + df[\"vs_p_BB\"]) / (df[\"vs_p_AB\"] + df[\"vs_p_BB\"] + df[\"vs_p_HP\"] + df[\"vs_p_SF\"])\n",
    "    new_df[\"vs_P_WHIP_RT\"] = (df[\"vs_p_P_WHIP_RT\"] / 24) \n",
    "    new_df[\"vs_OPS\"] = (new_df[\"vs_OBP\"] + new_df[\"vs_SLG\"])\n",
    "    new_df[\"vs_oOPS\"] = (new_df[\"vs_oOBP\"] + new_df[\"vs_oSLG\"])\n",
    "    new_df[\"vs_RC\"] = (((2.4 * vs_RC_C + vs_RC_A) * (3 * vs_RC_C + vs_RC_B)) / (9 * vs_RC_C) - (0.9 * vs_RC_C)) / 24\n",
    "    new_df[\"vs_GPA\"] = ((1.8 * new_df[\"vs_OBP\"] + new_df[\"vs_SLG\"]) / 4)\n",
    "    new_df[\"vs_SECA\"] = (((df[\"vs_h_H2\"] * 2) + (df[\"vs_h_H3\"] * 3) + (df[\"vs_h_HR\"] * 4) + df[\"vs_h_BB\"] + (df[\"vs_h_SB\"] - df[\"vs_h_CS\"])) / df[\"vs_h_AB\"])\n",
    "    new_df[\"vs_TA\"] = ((h_H1 + (df[\"vs_h_H2\"] * 2) + (df[\"vs_h_H3\"] * 3) + (df[\"vs_h_HR\"] * 4) + df[\"vs_h_HP\"] + df[\"vs_h_BB\"] + df[\"vs_h_SB\"]) / (df[\"vs_h_AB\"] - df[\"vs_h_HIT\"] + df[\"vs_h_CS\"] + df[\"vs_h_GD\"]))\n",
    "    new_df[\"vs_ISO\"] = (new_df[\"vs_SLG\"] - new_df[\"vs_AVG\"])\n",
    "    new_df[\"vs_oISO\"] = (new_df[\"vs_oSLG\"] - new_df[\"vs_oAVG\"])\n",
    "    new_df[\"vs_wOBA\"] = ((0.7 * (df[\"vs_h_BB\"] - df[\"vs_h_IB\"]) + (0.73 * df[\"vs_h_HP\"]) + (0.89 * vs_h_H1) + (1.27 * df[\"vs_h_H2\"]) + (1.61 * df[\"vs_h_H3\"]) + (2.07 * df[\"vs_h_HR\"]) + (0.25 * df[\"vs_h_SB\"]) -\n",
    "                (0.5 * df[\"vs_h_CS\"])) / (df[\"vs_h_AB\"] - df[\"vs_h_IB\"]))\n",
    "    new_df[\"vs_P_HRA_RT\"] = (df[\"vs_h_P_HRA_RT\"] / 24)\n",
    "    new_df[\"vs_DER\"] = ((df[\"vs_p_PA\"] - df[\"vs_p_HIT\"] - df[\"vs_p_KK\"] - df[\"vs_p_BB\"] - df[\"vs_p_HP\"] - df[\"vs_p_IB\"] - df[\"vs_p_BK\"]) / (df[\"vs_p_PA\"] - df[\"vs_p_HR\"] - df[\"vs_p_KK\"] - df[\"vs_p_BB\"] - df[\"vs_p_HP\"]))\n",
    "\n",
    "    \n",
    "    #new_df.to_csv(\"convert_temp.csv\") 파일로 변환해서 확인해봤습니다.\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2. 데이터 계산 및 합치기\n",
    "**실제 데이터에 대해 함수 적용**  \n",
    "\n",
    "데이터 개수가 적은 문제를 해결하기 위해 data augmentation을 적용하였다.  \n",
    "<br>\n",
    "Validation set으로 모든 시즌에 대해 각 팀의 후반부 24경기를 이용할 경우 모델에 후반부 데이터가 전혀 반영되지 않는 점이 우려되었다. 따라서 2020년도 데이터는 모두 train data로 사용하되, 2016~2019데이터를 1:3으로 분류하여 1을 train으로, 3의 후반부 데이터를 validatoin으로 이용해 총 4번의 교차검증을 진행하여 모델의 성능을 판단하기로 결정했다.\n",
    "<br><br>\n",
    "PCT에 누적없는 데이터를 이용하고, AVG/ERA에는 누적 데이터를 사용하므로 두 데이터 형식을 모두 계산하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2016\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2017\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2018\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2019\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n",
      "=== 2020\n",
      "LG\n",
      "HH\n",
      "NC\n",
      "HT\n",
      "SK\n",
      "KT\n",
      "WO\n",
      "LT\n",
      "SS\n",
      "OB\n"
     ]
    }
   ],
   "source": [
    "year = [2016,2017,2018,2019]\n",
    "orig_data = []\n",
    "cumul_data = []\n",
    "\n",
    "for y in year:\n",
    "    valid_y = year[:]\n",
    "    valid_y.remove(y)\n",
    "    \n",
    "    # 누적X\n",
    "    size_df_cal1 = calculate_df(size_df, valid_y, 24, 'size', ver='o')\n",
    "    sum_df_cal1 = calculate_df(sum_df, valid_y, 24, 'sum', ver='o')\n",
    "    merge_df1 = pd.merge(size_df_cal1, sum_df_cal1, how= 'left', left_on = ['T_ID',\"YEAR\",\"MERGE_IDX\",\"IDX\"], right_on  = ['T_ID',\"YEAR\",\"MERGE_IDX\",\"IDX\"])\n",
    "\n",
    "    conv1 = convert_attribute(merge_df1)\n",
    "    orig_data.append(conv1)\n",
    "    \n",
    "    # 누적O\n",
    "    size_df_cal2 = calculate_df(size_df, valid_y, 24, 'size', ver='c')\n",
    "    sum_df_cal2 = calculate_df(sum_df, valid_y, 24, 'sum', ver='c')\n",
    "    merge_df2 = pd.merge(size_df_cal2, sum_df_cal2, how= 'left', left_on = ['T_ID',\"YEAR\",\"MERGE_IDX\",\"IDX\"], right_on  = ['T_ID',\"YEAR\",\"MERGE_IDX\",\"IDX\"])\n",
    "    \n",
    "    conv2 = convert_attribute(merge_df2)\n",
    "    \n",
    "    cumul_data.append(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1023,
     "status": "ok",
     "timestamp": 1600447407294,
     "user": {
      "displayName": "‍엄세웅[학생](국제대학 국제학과)",
      "photoUrl": "",
      "userId": "01399474654548958341"
     },
     "user_tz": -540
    },
    "id": "IuTPVA6PGKYP"
   },
   "outputs": [],
   "source": [
    "# for i in range(len(orig_data)):\n",
    "#     orig_data[i].to_csv(\"orig_dataset_{}.csv\".format(i), index = False)\n",
    "#     cumul_data[i].to_csv(\"cumul_dataset_{}.csv\".format(i), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3. 누적 데이터 + 누적 AVG/ERA\n",
    "\n",
    "누적 데이터에 누적 타율/방어율 변수를 추가하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cumul_data = []\n",
    "\n",
    "for i in range(len(cumul_data)):\n",
    "    tmp_new = pd.concat([cumul_data[i],cteam_AVG_lst[i], cteam_ERA_lst[i]],axis=1)\n",
    "    new_cumul_data.append(tmp_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Valid/Test split\n",
    "**모델 학습을 위해 Train/Valid/Test 데이터 구성**  \n",
    "\n",
    "승률 예측 시 순수(비누적)데이터를 이용했고, 타율/방어율 예측 시 누적 데이터를 이용했다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifting(df):\n",
    "    sel_col = [\"T_ID\",\"IDX\",\"MERGE_IDX\",\"PCT\",\"ERA\",\"AVG\",'YEAR']\n",
    "    df_shift = df.loc[:,sel_col]\n",
    "    shift_col = sorted(list(set(list(df.columns))-set(['T_ID','YEAR'])))\n",
    "    \n",
    "    for i in range(1,2):\n",
    "        for c in shift_col:\n",
    "            df_shift.loc[:,'shift_{}'.format(c)] = df.loc[:,c].shift(i)\n",
    "    \n",
    "    df_shift.dropna(inplace=True)\n",
    "    \n",
    "    return df_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train 전처리  \n",
    "\n",
    "train 데이터를 구성하는 단계로, 이전 24경기(혹은 누적경기)를 이용해 다음 24경기의 승률(타율/방어율)을 계산해야 하므로 기본변수(T_ID, YEAR, (shift_)IDX, (shift_)MERGE_IDX)와 타겟 값(PCT/AVG/ERA)을 제외한 변수를 1칸씩 밀어주는 작업이 진행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_processing(df):\n",
    "    train = df[df[\"IDX\"]!=777]\n",
    "    \n",
    "    train_shift = shifting(train)\n",
    "    ts_idx = list(train_shift.index)\n",
    "    \n",
    "    drop_idx = []\n",
    "    for i in ts_idx:\n",
    "        idx_num = train_shift.loc[i,'IDX']\n",
    "        if idx_num <= train_shift.loc[i,'shift_IDX']:\n",
    "            drop_idx.append(i)\n",
    "    \n",
    "\n",
    "    train_s_df = train_shift.drop(drop_idx).reset_index(drop = True)  \n",
    "    train_s_drop_df = train_s_df.drop([\"IDX\",\"MERGE_IDX\",\"shift_IDX\",\"shift_MERGE_IDX\",\"ERA\",\"AVG\",\"PCT\"],axis = 1)\n",
    "\n",
    "    return train_s_df, train_s_drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train1 = []\n",
    "orig_train2 = []\n",
    "\n",
    "for i in range(len(orig_data)):\n",
    "    tmp_df = orig_data[i]\n",
    "    train_s_df, train_s_drop_df = train_processing(tmp_df)\n",
    "    orig_train1.append(train_s_df)\n",
    "    orig_train2.append(train_s_drop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul_train1 = []\n",
    "cumul_train2 = []\n",
    "\n",
    "for i in range(len(new_cumul_data)):    \n",
    "    tmp_df_c = new_cumul_data[i]\n",
    "    train_s_df_c, train_s_drop_df_c = train_processing(tmp_df_c)\n",
    "    cumul_train1.append(train_s_df_c)\n",
    "    cumul_train2.append(train_s_drop_df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Valid 전처리\n",
    "\n",
    "Valid 데이터를 구성하는 단계로, 미리 분류해둔 후반 24경기(IDX=777)의 바로 이전 데이터(IDX=5, 기준: 2016~2019)를 가져와 기본변수(T_ID, YEAR, (shift_)IDX, (shift_)MERGE_IDX)와 타겟 값(PCT/AVG/ERA)을 제외한 변수를 1칸씩 밀어주는 작업이 진행된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_processing(df, train_year):\n",
    "    test = df[df[\"IDX\"]==777]\n",
    "    tmp_valid = df[df[\"IDX\"]==5]\n",
    "    \n",
    "    tmp_valid = tmp_valid[(tmp_valid[\"YEAR\"] != str(train_year[0])) & (tmp_valid[\"YEAR\"] !=str(train_year[1]))]\n",
    "    \n",
    "    test = pd.concat([test,tmp_valid],axis=0)\n",
    "    test = test.sort_values(by=[\"YEAR\",\"T_ID\",\"IDX\"]).reset_index(drop=True)\n",
    "\n",
    "    test_shift = shifting(test)\n",
    "\n",
    "    test_s_df = test_shift[test_shift[\"IDX\"]==777].reset_index(drop=True)\n",
    "    test_s_drop_df = test_s_df.drop([\"IDX\",\"MERGE_IDX\",\"shift_IDX\",\"shift_MERGE_IDX\",\"ERA\",\"AVG\",\"PCT\"],axis = 1)\n",
    "    \n",
    "    return test_s_df, test_s_drop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_valid1 = []\n",
    "orig_valid2 = []\n",
    "\n",
    "for i in range(len(orig_data)):\n",
    "    tmp_df = orig_data[i]\n",
    "    valid_s_df, valid_s_drop_df = valid_processing(tmp_df,[i+2016,2020])\n",
    "    orig_valid1.append(valid_s_df)\n",
    "    orig_valid2.append(valid_s_drop_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul_valid1 = []\n",
    "cumul_valid2 = []\n",
    "\n",
    "for i in range(len(new_cumul_data)):    \n",
    "    tmp_df_c = new_cumul_data[i]\n",
    "    valid_s_df_c, valid_s_drop_df_c = valid_processing(tmp_df_c,[i+2016,2020])\n",
    "    cumul_valid1.append(valid_s_df_c)\n",
    "    cumul_valid2.append(valid_s_drop_df_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AVG/ERA dataset 가공  \n",
    "**타율과 방어율 예측에 사용할 수 있는 타자변수/투수변수 분류**\n",
    "<p>현재 Dataframe에는 T_ID(해당 팀), p_(해당 팀의 투수지표), h_(해당 팀의 타자지표), vs_p_(상대팀의 투수 관련지표), vs_h_(상대팀의 타자지표), 경기 정보 등으로 구성되어 있다. 타율과 방어율 예측 시 사용할 수 있는 변수가 다르기 때문에 이를 분류해주는 작업을 진행했다.\n",
    "\n",
    "- 타율(AVG) 예측시 사용 변수: 공통변수 + 타자변수 + 상대팀 투수변수\n",
    "- 방어율(ERA) 예측시 사용 변수: 공통변수 + 투수변수 + 상대팀 타자변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_col(train_col):\n",
    "    common_list = ['T_ID', 'YEAR', 'HEADER_NO','PCT']\n",
    "    hitter = ['AVG', 'OBP', 'SLG', 'OPS', 'RC', 'GPA', 'BA', 'SECA', \n",
    "              'TA', 'XR', 'ISO', 'wOBA', 'P_HRA_RT', 'DER', 'a_']\n",
    "    pitcher = ['WHIP', 'LOB', 'ERA', 'FIP', 'H_9', 'K_9', 'BB_9', \n",
    "               'HR_9', 'oAVG', 'oSLG', 'oOBP', 'P_WHIP_RT', 'P2_WHIP_RT', 'CB_WHIP_RT','oISO', 'oOPS','e_']\n",
    "\n",
    "    vs_hitter = ['vs_AVG', 'vs_OBP', 'vs_SLG', 'vs_OPS', 'vs_RC', 'vs_GPA', 'vs_BA', 'vs_SECA', \n",
    "                 'vs_TA', 'vs_XR', 'vs_ISO', 'vs_wOBA', 'vs_P_HRA_RT', 'vs_DER']\n",
    "    vs_pitcher = ['vs_WHIP', 'vs_LOB', 'vs_ERA', 'vs_FIP', 'vs_H_9', 'vs_K_9', 'vs_BB_9', \n",
    "                  'vs_HR_9', 'vs_oAVG', 'vs_oSLG', 'vs_oOBP', 'vs_P_WHIP_RT', 'vs_P2_WHIP_RT', 'vs_CB_WHIP_RT'\n",
    "                 ,'vs_oISO', 'vs_oOPS']\n",
    "\n",
    "    avg_col = common_list + hitter + vs_pitcher\n",
    "    era_col = common_list + pitcher + vs_hitter\n",
    "\n",
    "    sel_avg_lst =train_col[:2]\n",
    "    sel_era_lst = train_col[:2]\n",
    "\n",
    "    for i in train_col[2:]:\n",
    "        for k in avg_col:\n",
    "            if k in i:\n",
    "                sel_avg_lst.append(i)\n",
    "                break\n",
    "\n",
    "    for i in train_col[2:]:\n",
    "        for k in era_col:\n",
    "            if k in i:\n",
    "                sel_era_lst.append(i)\n",
    "                break\n",
    "                \n",
    "    return sel_avg_lst, sel_era_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col = list(cumul_train2[0].columns)\n",
    "sel_avg_lst, sel_era_lst = select_col(train_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 최종 데이터셋 생성 및 저장\n",
    "**입력변수와 타겟변수를 분리하여 각 예측값에 대한 입력 데이터셋 구축**  \n",
    "\n",
    "Augmentation이 완료된 데이터에 예측 그룹에 대한 홈경기수/상대팀구성비율을 추가하여 최종 입력변수(X)를 생성한 후, 타겟변수(y)를 분리한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 생성(Train, Valid)\n",
    "- PCT(누적 안한 데이터 이용): PCT_train_X, PCT_train_y, PCT_valid_X, PCT_valid_y\n",
    "- AVG(누적 데이터 이용): AVG_train_X, AVG_train_y, AVG_valid_X, AVG_valid_y\n",
    "- ERA(누적 데이터 이용): ERA_train_X, ERA_train_y, ERA_valid_X, ERA_valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvs_col = ['c_HH', 'c_HT', 'c_KT', 'c_LG', 'c_LT', 'c_NC', 'c_OB', 'c_SK', 'c_SS', 'c_WO',\"ISHOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 승률\n",
    "PCT_train_X_lst = []\n",
    "PCT_train_y_lst = []\n",
    "\n",
    "PCT_valid_X_lst = []\n",
    "PCT_valid_y_lst = []\n",
    "\n",
    "for i in range(len(orig_train1)):\n",
    "    hvs_tmp = Home_VS_lst[i]\n",
    "    tX = hvs_tmp[(hvs_tmp[\"IDX\"] !=777)&(hvs_tmp[\"IDX\"] !=1)].loc[:,hvs_col].reset_index(drop=True)\n",
    "    vX = hvs_tmp[hvs_tmp[\"IDX\"]==777].loc[:,hvs_col].reset_index(drop=True)\n",
    "\n",
    "    PCT_train_X = orig_train2[i]\n",
    "    PCT_train_y = orig_train1[i][[\"T_ID\",\"YEAR\",\"PCT\"]]\n",
    "    \n",
    "    PCT_valid_X = orig_valid2[i]\n",
    "    PCT_valid_y = orig_valid1[i][[\"T_ID\",\"YEAR\",\"PCT\"]]\n",
    "    \n",
    "    PCT_train_X_lst.append(pd.concat([PCT_train_X,tX],axis=1))\n",
    "    PCT_train_y_lst.append(PCT_train_y)\n",
    "    \n",
    "    PCT_valid_X_lst.append(pd.concat([PCT_valid_X,vX],axis=1))\n",
    "    PCT_valid_y_lst.append(PCT_valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타율\n",
    "AVG_train_X_lst = []\n",
    "AVG_train_y_lst = []\n",
    "\n",
    "AVG_valid_X_lst = []\n",
    "AVG_valid_y_lst = []\n",
    "\n",
    "for i in range(len(cumul_train1)):\n",
    "    hvs_tmp = Home_VS_lst[i]\n",
    "    tX = hvs_tmp[(hvs_tmp[\"IDX\"] !=777)&(hvs_tmp[\"IDX\"] !=1)].loc[:,hvs_col].reset_index(drop=True)\n",
    "    vX = hvs_tmp[hvs_tmp[\"IDX\"]==777].loc[:,hvs_col].reset_index(drop=True)\n",
    "\n",
    "    AVG_train_X = cumul_train2[i][sel_avg_lst]\n",
    "    AVG_train_y = orig_train1[i][[\"T_ID\",\"YEAR\",\"AVG\"]]\n",
    "    \n",
    "    AVG_valid_X = cumul_valid2[i][sel_avg_lst]\n",
    "    AVG_valid_y = orig_valid1[i][[\"T_ID\",\"YEAR\",\"AVG\"]]\n",
    "    \n",
    "    AVG_train_X_lst.append(pd.concat([AVG_train_X,tX],axis=1))\n",
    "    AVG_train_y_lst.append(AVG_train_y)\n",
    "    \n",
    "    AVG_valid_X_lst.append(pd.concat([AVG_valid_X,vX],axis=1))\n",
    "    AVG_valid_y_lst.append(AVG_valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방어율\n",
    "ERA_train_X_lst = []\n",
    "ERA_train_y_lst = []\n",
    "\n",
    "ERA_valid_X_lst = []\n",
    "ERA_valid_y_lst = []\n",
    "\n",
    "for i in range(len(cumul_train1)):\n",
    "    hvs_tmp = Home_VS_lst[i]\n",
    "    tX = hvs_tmp[(hvs_tmp[\"IDX\"] !=777)&(hvs_tmp[\"IDX\"] !=1)].loc[:,hvs_col].reset_index(drop=True)\n",
    "    vX = hvs_tmp[hvs_tmp[\"IDX\"]==777].loc[:,hvs_col].reset_index(drop=True)\n",
    "\n",
    "    ERA_train_X = cumul_train2[i][sel_era_lst]\n",
    "    ERA_train_y = orig_train1[i][[\"T_ID\",\"YEAR\",\"ERA\"]]\n",
    "    \n",
    "    ERA_valid_X = cumul_valid2[i][sel_era_lst]\n",
    "    ERA_valid_y = orig_valid1[i][[\"T_ID\",\"YEAR\",\"ERA\"]]\n",
    "    \n",
    "    ERA_train_X_lst.append(pd.concat([ERA_train_X,tX],axis=1))\n",
    "    ERA_train_y_lst.append(ERA_train_y)\n",
    "    \n",
    "    ERA_valid_X_lst.append(pd.concat([ERA_valid_X,vX],axis=1))\n",
    "    ERA_valid_y_lst.append(ERA_valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3858, 53), (3858, 3), (30, 53), (30, 3))"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ERA_train_X.shape, ERA_train_y.shape, ERA_valid_X.shape, ERA_valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3858, 56), (3858, 3), (30, 56), (30, 3))"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVG_train_X.shape, AVG_train_y.shape, AVG_valid_X.shape, AVG_valid_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3858, 55), (3858, 3), (30, 55), (30, 3))"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCT_train_X.shape, PCT_train_y.shape, PCT_valid_X.shape, PCT_valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 생성(Test)\n",
    "**정규시즌 잔여 경기에 대한 각 팀별 승률, 타율 및 방어율(평균자책점) 예측을 위한 입력값 생성**\n",
    "\n",
    "- PCT(누적 안한 데이터 이용): PCT_test_X\n",
    "- AVG(누적 데이터 이용): AVG_test_X\n",
    "- ERA(누적 데이터 이용): ERA_test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 잔여경기 홈/상대팀 비율\n",
    "last_game = pd.read_csv(\"../data/2020_잔여경기.csv\", encoding=\"utf-8-sig\") \n",
    "team_name = pd.read_csv(\"../data/2020빅콘테스트_스포츠투아이_제공데이터_팀_2020.csv\", encoding=\"euc-kr\")\n",
    "\n",
    "tmp_g1 = pd.merge(last_game, team_name, how= 'left', left_on=[\"HOME\"], right_on=[\"T_NM\"])\n",
    "tmp_g2 = pd.merge(tmp_g1, team_name, how= 'left', left_on=[\"AWAY\"], right_on=[\"T_NM\"])[[\"T_ID_x\",\"T_ID_y\"]]\n",
    "\n",
    "tmp_alt_g2 = tmp_g2.copy()\n",
    "tmp_alt_g2 = tmp_alt_g2.rename(columns = {'T_ID_x':'T_ID_y','T_ID_y':\"T_ID_x\"})\n",
    "\n",
    "tmp_g2[\"ISHOME\"]=1\n",
    "tmp_alt_g2[\"ISHOME\"] = 0\n",
    "\n",
    "tmp2020_last = pd.concat([tmp_g2, tmp_alt_g2], axis= 0).rename(columns = {'T_ID_x':'T_ID','T_ID_y':\"VS_T_ID\"})\n",
    "\n",
    "df2020_new = pd.concat([tmp2020_last, pd.get_dummies(tmp2020_last.VS_T_ID)], axis = 1).reset_index(drop = True)\n",
    "df2020_new_df = make_sum(df2020_new, 'home')\n",
    "\n",
    "df2020_new_df.iloc[:,-11:] = df2020_new_df.iloc[:,-11:].astype(np.int64)\n",
    "\n",
    "t2020_cal = df2020_new_df.add_prefix('c_').iloc[:,-10:]\n",
    "\n",
    "for i in range(len(df2020_new_df)):\n",
    "    t2020_cal.iloc[i,:] = t2020_cal.iloc[i,:]/sum(t2020_cal.iloc[i,:])\n",
    "    \n",
    "df_team_2020 = pd.concat([df2020_new_df.iloc[:,:2], t2020_cal],axis=1).sort_values(by=[\"T_ID\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCT, AVG, ERA 따로\n",
    "size_2020_df = size_df[size_df[\"YEAR\"]=='2020'].drop([\"GDAY_DS\"],axis=1)\n",
    "sum_2020_df = sum_df[sum_df[\"YEAR\"]=='2020'].drop([\"GDAY_DS\"],axis=1)\n",
    "\n",
    "test_orig_df = pd.DataFrame([])\n",
    "test_cumul_df = pd.DataFrame([])\n",
    "\n",
    "key = 24\n",
    "for t in team:\n",
    "    ttmp_size_df = size_2020_df[size_2020_df[\"T_ID\"]==t]\n",
    "    ttmp_sum_df = sum_2020_df[sum_2020_df[\"T_ID\"]==t]\n",
    "    tmp_size = len(ttmp_size_df)\n",
    "    \n",
    "    # 누적 X\n",
    "    ttmp_size_test_df = ttmp_size_df.iloc[tmp_size-key:,:]\n",
    "    ttmp_sum_test_df = ttmp_sum_df.iloc[tmp_size-key:,:]\n",
    "    \n",
    "    tmp1 = make_pct(ttmp_size_test_df)[[\"PCT\"]]\n",
    "    tmp2 = make_sum(ttmp_sum_test_df)\n",
    "    \n",
    "    merge_df1 = pd.concat([tmp2,tmp1], axis=1)\n",
    "    conv1 = convert_attribute(merge_df1, 'test')\n",
    "    \n",
    "    test_orig_df = pd.concat([test_orig_df,conv1],axis=0)\n",
    "    \n",
    "    # 누적 O\n",
    "    ttmp_size_test_df_c = ttmp_size_df.iloc[:tmp_size-key,:]\n",
    "    ttmp_sum_test_df_c = ttmp_sum_df.iloc[:tmp_size-key,:]\n",
    "    \n",
    "    tmp1_c = make_pct(ttmp_size_test_df_c)[[\"PCT\"]]\n",
    "    tmp2_c = make_sum(ttmp_sum_test_df_c)\n",
    "    \n",
    "    merge_df2 = pd.concat([tmp2_c,tmp1_c], axis=1)\n",
    "    conv2 = convert_attribute(merge_df2, 'test')\n",
    "    \n",
    "    test_cumul_df = pd.concat([test_cumul_df,conv2],axis=0)\n",
    "\n",
    "test_orig_df = test_orig_df.reset_index(drop=True)  \n",
    "test_cumul_df = test_cumul_df.reset_index(drop=True)\n",
    "    \n",
    "# 누적 승타방\n",
    "ae_team_df = sum_2020_df[def_col + ae_col]\n",
    "ae_team_df_c = make_sum(ae_team_df, 'vs')\n",
    "\n",
    "ae_team_df_c[\"ERA\"] = (9 * ae_team_df_c[\"p_ER\"]) / (ae_team_df_c[\"p_INN2\"] / 3)\n",
    "ae_team_df_c[\"AVG\"] = (ae_team_df_c[\"h_HIT\"] / ae_team_df_c[\"h_AB\"])\n",
    "\n",
    "cul_t_avg_df_c = ae_team_df_c.drop([\"ERA\"]+ae_col,axis=1)\n",
    "cul_t_era_df_c = ae_team_df_c.drop([\"AVG\"]+ae_col,axis=1)\n",
    "\n",
    "cul_avg_df_c = cul_t_avg_df_c.pivot_table('AVG', ['T_ID', 'YEAR'], 'VS_T_ID').reset_index().fillna(0)\n",
    "cul_era_df_c = cul_t_era_df_c.pivot_table('ERA', ['T_ID', 'YEAR'], 'VS_T_ID').reset_index().fillna(0)\n",
    "\n",
    "ca_c = cul_avg_df_c.iloc[:,-10:].add_prefix('a_')\n",
    "ce_c = cul_era_df_c.iloc[:,-10:].add_prefix('e_')\n",
    "\n",
    "test_cumul_df_c = pd.concat([test_cumul_df,ca_c,ce_c],axis=1)\n",
    "\n",
    "\n",
    "test_col = list(test_cumul_df_c.columns)\n",
    "sel_avg_lst_t, sel_era_lst_t = select_col(test_col)\n",
    "\n",
    "PCT_test_X = test_orig_df.copy()\n",
    "AVG_test_X = test_cumul_df_c[sel_avg_lst_t]\n",
    "ERA_test_X = test_cumul_df_c[sel_era_lst_t]\n",
    "    \n",
    "PCT_test_X_fin = pd.concat([PCT_test_X, df_team_2020.iloc[:,1:]], axis = 1 )\n",
    "AVG_test_X_fin = pd.concat([AVG_test_X, df_team_2020.iloc[:,1:]], axis = 1 )\n",
    "ERA_test_X_fin = pd.concat([ERA_test_X, df_team_2020.iloc[:,1:]], axis = 1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_test_col(orig_col, test_df):\n",
    "    tmp_col = [name.replace(\"shift_\", \"\") for name in orig_col]\n",
    "    new_test_df = test_df[tmp_col]\n",
    "    \n",
    "    mod_col = [\"T_ID\",\"YEAR\"]\n",
    "    mod_col += [\"shift_\" + name for name in tmp_col[2:-11]]\n",
    "    mod_col += tmp_col[-11:]\n",
    "    \n",
    "    return new_test_df, mod_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_temp_column = PCT_train_X_lst[0].columns.to_list()\n",
    "PCT_test_X_set = PCT_test_X_fin.copy()\n",
    "PCT_test_X_set, PCT_test_X_set.columns = rename_test_col(p_temp_column, PCT_test_X_set)\n",
    "\n",
    "a_temp_column = AVG_train_X_lst[0].columns.to_list()\n",
    "AVG_test_X_set = AVG_test_X_fin.copy()\n",
    "AVG_test_X_set, AVG_test_X_set.columns = rename_test_col(a_temp_column, AVG_test_X_set)\n",
    "\n",
    "e_temp_column = ERA_train_X_lst[0].columns.to_list()\n",
    "ERA_test_X_set = ERA_test_X_fin.copy()\n",
    "ERA_test_X_set,ERA_test_X_set.columns = rename_test_col(e_temp_column, ERA_test_X_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 66), (10, 67), (10, 64))"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCT_test_X_set.shape, AVG_test_X_set.shape, ERA_test_X_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3858, 66), (3858, 67), (3858, 64))"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCT_train_X_lst[0].shape, AVG_train_X_lst[0].shape,ERA_train_X_lst[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 저장  \n",
    "\n",
    "전처리가 완료된 데이터를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(PCT_train_X_lst)):\n",
    "    PCT_train_X_lst[i].to_csv(\"../data/PCT/PCT_train_X_{}.csv\".format(i+1),index = False)\n",
    "    PCT_train_y_lst[i].to_csv(\"../data/PCT/PCT_train_y_{}.csv\".format(i+1),index = False)\n",
    "\n",
    "    PCT_valid_X_lst[i].to_csv(\"../data/PCT/PCT_valid_X_{}.csv\".format(i+1),index = False)\n",
    "    PCT_valid_y_lst[i].to_csv(\"../data/PCT/PCT_valid_y_{}.csv\".format(i+1),index = False)\n",
    "\n",
    "    AVG_train_X_lst[i].to_csv(\"../data/AVG/AVG_train_X_{}.csv\".format(i+1),index = False)\n",
    "    AVG_train_y_lst[i].to_csv(\"../data/AVG/AVG_train_y_{}.csv\".format(i+1),index = False)\n",
    "\n",
    "    AVG_valid_X_lst[i].to_csv(\"../data/AVG/AVG_valid_X_{}.csv\".format(i+1),index = False)\n",
    "    AVG_valid_y_lst[i].to_csv(\"../data/AVG/AVG_valid_y_{}.csv\".format(i+1),index = False)\n",
    "    \n",
    "    ERA_train_X_lst[i].to_csv(\"../data/ERA/ERA_train_X_{}.csv\".format(i+1),index = False)\n",
    "    ERA_train_y_lst[i].to_csv(\"../data/ERA/ERA_train_y_{}.csv\".format(i+1),index = False)\n",
    "\n",
    "    ERA_valid_X_lst[i].to_csv(\"../data/ERA/ERA_valid_X_{}.csv\".format(i+1),index = False)\n",
    "    ERA_valid_y_lst[i].to_csv(\"../data/ERA/ERA_valid_y_{}.csv\".format(i+1),index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCT_test_X_set.to_csv(\"../data/test/PCT_test_X.csv\",index = False)\n",
    "AVG_test_X_set.to_csv(\"../data/test/AVG_test_X.csv\",index = False)\n",
    "ERA_test_X_set.to_csv(\"../data/test/ERA_test_X.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Preprocess.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
