{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data: augment_24group_1620.csv필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCT_lstm_train_X = pd.read_csv(\"AVG_lstm_final_train_X_hit.csv\")\n",
    "# PCT_lstm_train_y = pd.read_csv(\"AVG_lstm_final_train_y_hit.csv\")\n",
    "\n",
    "# PCT_lstm_test_X = pd.read_csv(\"AVG_lstm_final_test_X_hit.csv\")\n",
    "# PCT_lstm_test_y = pd.read_csv(\"AVG_lstm_final_test_y_hit.csv\")\n",
    "\n",
    "lstm_train_X = pd.read_csv(\"lstm_data/new_AVG_train_X.csv\")\n",
    "lstm_train_y = pd.read_csv(\"lstm_data/AVG_lstm_final_train_y.csv\")\n",
    "\n",
    "lstm_test_X = pd.read_csv(\"lstm_data/new_AVG_test_X.csv\")\n",
    "lstm_test_y = pd.read_csv(\"lstm_data/AVG_lstm_final_test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = list(lstm_train_X.T_ID.unique())\n",
    "year = list(lstm_train_y.YEAR.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['LG', 'HH', 'NC', 'HT', 'SK', 'KT', 'WO', 'LT', 'SS', 'OB'],\n",
       " [2016, 2017, 2018, 2019])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team, year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) input shape로 변경 (row, timestep=2, feature)\n",
    "\n",
    "ex) \n",
    "timestep = 2\n",
    "\n",
    "* X_train_v 구성예시: [[1 ~ 24경기 데이터, 25 ~ 48경기 데이터], [49 ~ 72경기 데이터, 73 ~ 96경기 데이터] ]  \n",
    "X_train_v.shape >> (2,2*x)             # x: 각 24group에 대한 변수 개수\n",
    "* y_train_v 구성예시: 97 ~ 120 경기 승률\n",
    "\n",
    "=> reshape\n",
    "\n",
    "* X_train_v.shape >> (2,2,x)  # row, timestep, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 구성(LSTM)\n",
    "- optimizer: RMSprop -> lr(learning rate) 조절\n",
    "- LSTM: 모델이 계속 동일한 결과값이 나올 때, input 뉴런 개수를 늘려야 한다는 글을 읽고 계속 input 노드 개수를 바꿔주면서 모델 생성중\n",
    "- loss: MSE\n",
    "\n",
    "- early_stop: patience를 크게하면 과적합 되는 경우가 있어서 최대한 작게 설정해둠\n",
    "- batch_size: 모델이 계속 동일한 결과값이 나올 때, 데이터가 적어 batch size를 줄여보라는 글을 읽고 1로 설정해둠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016LG =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 3s 52ms/sample - loss: 0.5085 - mae: 0.2563\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.6049e-04 - mae: 0.0102\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.4169e-04 - mae: 0.0104\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.2442e-04 - mae: 0.0098\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.3103e-04 - mae: 0.0101\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.4424e-04 - mae: 0.0105\n",
      "Epoch 00006: early stopping\n",
      "2016HH =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 3s 55ms/sample - loss: 0.3228 - mae: 0.2311\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.7079e-04 - mae: 0.0097\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1466e-04 - mae: 0.0089\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.5151e-04 - mae: 0.0139\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2648e-04 - mae: 0.0091\n",
      "Epoch 00005: early stopping\n",
      "2016NC =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 34ms/sample - loss: 0.3753 - mae: 0.2142\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.9874e-04 - mae: 0.0163\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.1832e-04 - mae: 0.0119\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.9495e-04 - mae: 0.0115\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.7415e-04 - mae: 0.0111\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.7277e-04 - mae: 0.0108\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.9098e-04 - mae: 0.0135\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8419e-04 - mae: 0.0111\n",
      "Epoch 00008: early stopping\n",
      "2016HT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 30ms/sample - loss: 0.2612 - mae: 0.2033\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8480e-04 - mae: 0.0105\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1071e-04 - mae: 0.0080\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 9.2817e-05 - mae: 0.0072\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0878e-04 - mae: 0.0081\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.1671e-04 - mae: 0.0082\n",
      "Epoch 00006: early stopping\n",
      "2016SK =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 31ms/sample - loss: 0.3465 - mae: 0.2713\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.5141e-04 - mae: 0.0180\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.7265e-04 - mae: 0.0143\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.5198e-04 - mae: 0.0135\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.2274e-04 - mae: 0.0125\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.1864e-04 - mae: 0.0124\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.6273e-04 - mae: 0.0105\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.6431e-04 - mae: 0.0107\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.3304e-04 - mae: 0.0090\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0617e-04 - mae: 0.0086\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.1861e-04 - mae: 0.0087\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.9373e-04 - mae: 0.0111\n",
      "Epoch 00012: early stopping\n",
      "2016KT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 31ms/sample - loss: 0.4042 - mae: 0.2444\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.4559e-04 - mae: 0.0122\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.7319e-04 - mae: 0.0100\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.3639e-04 - mae: 0.0098\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.1190e-04 - mae: 0.0088\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0446e-04 - mae: 0.0082\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.9923e-04 - mae: 0.0110\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1006e-04 - mae: 0.0087\n",
      "Epoch 00008: early stopping\n",
      "2016WO =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 35ms/sample - loss: 0.4447 - mae: 0.2818\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.7210e-04 - mae: 0.0131\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 2.2120e-04 - mae: 0.0124\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.8809e-04 - mae: 0.0107\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3811e-04 - mae: 0.0096\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5394e-04 - mae: 0.0106\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2029e-04 - mae: 0.0093\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2860e-04 - mae: 0.0095\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2288e-04 - mae: 0.0090\n",
      "Epoch 00009: early stopping\n",
      "2016LT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 48ms/sample - loss: 0.4069 - mae: 0.3159\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.9234e-04 - mae: 0.0200\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 4.6662e-04 - mae: 0.0184\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.0104e-04 - mae: 0.0190\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 4.3424e-04 - mae: 0.0179\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 5.7059e-04 - mae: 0.0204\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 4.7038e-04 - mae: 0.0179\n",
      "Epoch 00007: early stopping\n",
      "2016SS =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 50ms/sample - loss: 0.1950 - mae: 0.1947\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 4.2692e-04 - mae: 0.0182 0s - loss: 4.9784e-04 - mae: 0.020\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.3841e-04 - mae: 0.0179\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 3.7629e-04 - mae: 0.0160\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 3.9970e-04 - mae: 0.0159\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 3.3149e-04 - mae: 0.0158\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.5800e-04 - mae: 0.0135\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.8679e-04 - mae: 0.0142\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.4939e-04 - mae: 0.0137\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.6711e-04 - mae: 0.0137\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.0332e-04 - mae: 0.0118\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.2889e-04 - mae: 0.0183\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.4758e-04 - mae: 0.0126\n",
      "Epoch 00013: early stopping\n",
      "2016OB =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 46ms/sample - loss: 0.4899 - mae: 0.2530\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.4223e-04 - mae: 0.0092\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 7.5241e-05 - mae: 0.0071\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 7.5054e-05 - mae: 0.0067\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.4221e-04 - mae: 0.0095\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 8.6684e-05 - mae: 0.0075\n",
      "Epoch 00006: early stopping\n",
      "2017LG =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 47ms/sample - loss: 0.3512 - mae: 0.2778\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.9677e-04 - mae: 0.0194\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.1924e-04 - mae: 0.0116\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.3079e-04 - mae: 0.0125\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.9391e-04 - mae: 0.0108\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.7415e-04 - mae: 0.0136\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.3592e-04 - mae: 0.0087\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.4394e-04 - mae: 0.0096\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.1982e-04 - mae: 0.0084\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.6350e-04 - mae: 0.0098\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.6788e-04 - mae: 0.0108\n",
      "Epoch 00011: early stopping\n",
      "2017HH =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 36ms/sample - loss: 0.3685 - mae: 0.2158\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8099e-04 - mae: 0.0111\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3782e-04 - mae: 0.0099\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.5736e-04 - mae: 0.0108\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.3936e-04 - mae: 0.0098\n",
      "Epoch 00005: early stopping\n",
      "2017NC =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 38ms/sample - loss: 0.4278 - mae: 0.2798\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 6.4999e-04 - mae: 0.0217\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.6131e-04 - mae: 0.0163\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.6848e-04 - mae: 0.0178\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.3340e-04 - mae: 0.0184\n",
      "Epoch 00005: early stopping\n",
      "2017HT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 47ms/sample - loss: 0.4124 - mae: 0.2329\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 6.4111e-04 - mae: 0.0216\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.0939e-04 - mae: 0.0254\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.4291e-04 - mae: 0.0201\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.5826e-04 - mae: 0.0179\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 6.4570e-04 - mae: 0.0220\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.9391e-04 - mae: 0.0177\n",
      "Epoch 00007: early stopping\n",
      "2017SK =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 30ms/sample - loss: 0.2313 - mae: 0.2103\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.4361e-04 - mae: 0.0172\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - ETA: 0s - loss: 2.0955e-04 - mae: 0.012 - 0s 2ms/sample - loss: 2.0033e-04 - mae: 0.0117\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.3500e-04 - mae: 0.0099\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3345e-04 - mae: 0.0096\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.3430e-04 - mae: 0.0098\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2868e-04 - mae: 0.0095\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.0440e-04 - mae: 0.0087\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.1138e-04 - mae: 0.0089\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5474e-04 - mae: 0.0105\n",
      "Epoch 00010: early stopping\n",
      "2017KT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 38ms/sample - loss: 0.2660 - mae: 0.2075\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.2701e-04 - mae: 0.0091\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 7.7359e-05 - mae: 0.0075\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 7.4850e-05 - mae: 0.0069\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 7.1941e-05 - mae: 0.0071\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 7.2780e-05 - mae: 0.0069\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 9.2349e-05 - mae: 0.0075\n",
      "Epoch 00007: early stopping\n",
      "2017WO =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 29ms/sample - loss: 0.3460 - mae: 0.2420\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.6214e-04 - mae: 0.0159\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.0977e-04 - mae: 0.0170\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 3.0103e-04 - mae: 0.0146\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 3.3253e-04 - mae: 0.0157\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 4.2975e-04 - mae: 0.0159\n",
      "Epoch 00006: early stopping\n",
      "2017LT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 31ms/sample - loss: 0.3208 - mae: 0.2088\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8876e-04 - mae: 0.0115\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.5784e-04 - mae: 0.0102\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0705e-04 - mae: 0.0077\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5732e-04 - mae: 0.0103\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5062e-04 - mae: 0.0099\n",
      "Epoch 00006: early stopping\n",
      "2017SS =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 37ms/sample - loss: 0.5012 - mae: 0.2578\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.1343e-04 - mae: 0.0114\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.1635e-04 - mae: 0.0086\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.8854e-04 - mae: 0.0107\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.5128e-04 - mae: 0.0100\n",
      "Epoch 00005: early stopping\n",
      "2017OB =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 39ms/sample - loss: 0.3296 - mae: 0.2093\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.1171e-04 - mae: 0.0120\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.7251e-04 - mae: 0.0109\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4825e-04 - mae: 0.0100\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.8884e-04 - mae: 0.0114\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4879e-04 - mae: 0.0101\n",
      "Epoch 00006: early stopping\n",
      "2018LG =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 29ms/sample - loss: 0.3018 - mae: 0.2344\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 3.2102e-04 - mae: 0.0148\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1822e-04 - mae: 0.0088\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3410e-04 - mae: 0.0090\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0115e-04 - mae: 0.0084\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4045e-04 - mae: 0.0096\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.4338e-04 - mae: 0.0096\n",
      "Epoch 00007: early stopping\n",
      "2018HH =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 29ms/sample - loss: 0.2629 - mae: 0.2484\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.9567e-04 - mae: 0.0136\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.8039e-04 - mae: 0.0110\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.9235e-04 - mae: 0.0118\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8597e-04 - mae: 0.0111\n",
      "Epoch 00005: early stopping\n",
      "2018NC =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 28ms/sample - loss: 0.3974 - mae: 0.2752\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 7.4150e-04 - mae: 0.0210\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.4668e-04 - mae: 0.0099\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2408e-04 - mae: 0.0089\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 0s 2ms/sample - loss: 6.6178e-05 - mae: 0.0066\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 5.9881e-05 - mae: 0.0061\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 6.0865e-05 - mae: 0.0065\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 5.8869e-05 - mae: 0.0061\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 5.1811e-05 - mae: 0.0061\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 6.2272e-05 - mae: 0.0067\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 8.8120e-05 - mae: 0.0076\n",
      "Epoch 00011: early stopping\n",
      "2018HT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 44ms/sample - loss: 0.3475 - mae: 0.2608\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.1593e-04 - mae: 0.0124\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3848e-04 - mae: 0.0098\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.3767e-04 - mae: 0.0096\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5054e-04 - mae: 0.0101\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0530e-04 - mae: 0.0083\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.7852e-05 - mae: 0.0078\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.7040e-05 - mae: 0.0078\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 6.6606e-05 - mae: 0.0065\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 8.8805e-05 - mae: 0.0077\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.0413e-04 - mae: 0.0086\n",
      "Epoch 00011: early stopping\n",
      "2018SK =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 32ms/sample - loss: 0.2929 - mae: 0.2376\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.4415e-04 - mae: 0.0167\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3749e-04 - mae: 0.0103\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5626e-04 - mae: 0.0105\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.6688e-05 - mae: 0.0080\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8954e-04 - mae: 0.0114\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.2796e-04 - mae: 0.0094\n",
      "Epoch 00007: early stopping\n",
      "2018KT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 31ms/sample - loss: 0.3469 - mae: 0.2484\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.0829e-04 - mae: 0.0124\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.1318e-04 - mae: 0.0120\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 2.1039e-04 - mae: 0.0119\n",
      "Epoch 00004: early stopping\n",
      "2018WO =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 50ms/sample - loss: 0.2787 - mae: 0.2003\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 8.0340e-04 - mae: 0.0229\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 6.9229e-04 - mae: 0.0201\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.4713e-04 - mae: 0.0176\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.8391e-04 - mae: 0.0189\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.3492e-04 - mae: 0.0191\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 0.0014 - mae: 0.0306\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 6.9196e-04 - mae: 0.0214\n",
      "Epoch 00008: early stopping\n",
      "2018LT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 32ms/sample - loss: 0.2675 - mae: 0.2422\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.1021e-04 - mae: 0.0118\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.9544e-05 - mae: 0.0061\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 7.7843e-05 - mae: 0.0071\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.6365e-05 - mae: 0.0080\n",
      "Epoch 00005: early stopping\n",
      "2018SS =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 44ms/sample - loss: 0.4903 - mae: 0.3345\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.4382e-04 - mae: 0.0150\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.2770e-04 - mae: 0.0093\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2879e-04 - mae: 0.0090\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2877e-04 - mae: 0.0091\n",
      "Epoch 00005: early stopping\n",
      "2018OB =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 31ms/sample - loss: 0.1923 - mae: 0.1520\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.3119e-04 - mae: 0.0134\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 2.6508e-04 - mae: 0.0140\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 3.3477e-04 - mae: 0.0143\n",
      "Epoch 00004: early stopping\n",
      "2019LG =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 30ms/sample - loss: 0.3065 - mae: 0.1961\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.4421e-04 - mae: 0.0121\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8516e-04 - mae: 0.0116\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.9103e-04 - mae: 0.0115\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.8892e-04 - mae: 0.0115\n",
      "Epoch 00005: early stopping\n",
      "2019HH =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 29ms/sample - loss: 0.3787 - mae: 0.2494\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.0064e-04 - mae: 0.0111\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.5403e-05 - mae: 0.0083\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 1.0503e-04 - mae: 0.0089\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 8.3458e-05 - mae: 0.0079\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.5546e-05 - mae: 0.0083\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5363e-04 - mae: 0.0096\n",
      "Epoch 00007: early stopping\n",
      "2019NC =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 31ms/sample - loss: 0.3633 - mae: 0.2419\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.8558e-04 - mae: 0.0193\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 6.0135e-04 - mae: 0.0207\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.4384e-04 - mae: 0.0197\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.1922e-04 - mae: 0.0198\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 6.5441e-04 - mae: 0.0213\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.4333e-04 - mae: 0.0173\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 4.4064e-04 - mae: 0.0171\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.6208e-04 - mae: 0.0171\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 4.2395e-04 - mae: 0.0162\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.3539e-04 - mae: 0.0154\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.0140e-04 - mae: 0.0117\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.8869e-04 - mae: 0.0145\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5631e-04 - mae: 0.0100\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.0690e-04 - mae: 0.0079\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 0s 6ms/sample - loss: 1.4772e-04 - mae: 0.0100\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.1417e-04 - mae: 0.0084\n",
      "Epoch 00017: early stopping\n",
      "2019HT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 3s 54ms/sample - loss: 0.3642 - mae: 0.2438\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.1817e-05 - mae: 0.0060\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 4.3785e-05 - mae: 0.0053\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.4627e-05 - mae: 0.0060\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 5.0586e-05 - mae: 0.0062\n",
      "Epoch 00005: early stopping\n",
      "2019SK =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 3s 54ms/sample - loss: 0.4401 - mae: 0.2832\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 3.2062e-04 - mae: 0.0144\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.5959e-04 - mae: 0.0105\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4885e-04 - mae: 0.0098\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.6299e-04 - mae: 0.0106\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2568e-04 - mae: 0.0089\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3951e-04 - mae: 0.0098\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.2914e-04 - mae: 0.0097\n",
      "Epoch 00008: early stopping\n",
      "2019KT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 30ms/sample - loss: 0.3488 - mae: 0.2780\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.9487e-04 - mae: 0.0101\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5212e-04 - mae: 0.0098\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 1.0653e-04 - mae: 0.0085\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1776e-04 - mae: 0.0084\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 9.7122e-05 - mae: 0.0072\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 7.5469e-05 - mae: 0.0070\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 8.7889e-05 - mae: 0.0077\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1315e-04 - mae: 0.0083\n",
      "Epoch 00009: early stopping\n",
      "2019WO =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 33ms/sample - loss: 0.3022 - mae: 0.2287\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 2ms/sample - loss: 9.2978e-05 - mae: 0.0081\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.4586e-05 - mae: 0.0059\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.0000e-05 - mae: 0.0054\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 6.0443e-05 - mae: 0.0061\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 5.2792e-05 - mae: 0.0058\n",
      "Epoch 00006: early stopping\n",
      "2019LT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 2s 31ms/sample - loss: 0.3182 - mae: 0.2329\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.8452e-04 - mae: 0.0162\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.7292e-04 - mae: 0.0140\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.3363e-04 - mae: 0.0127\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.0962e-04 - mae: 0.0127\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5953e-04 - mae: 0.0101\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4860e-04 - mae: 0.0102\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.0754e-04 - mae: 0.0079\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1744e-04 - mae: 0.0084\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.0093e-04 - mae: 0.0080\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3499e-04 - mae: 0.0091\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 8.4886e-05 - mae: 0.0075\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 8.0577e-05 - mae: 0.0072\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 8.2462e-05 - mae: 0.0071\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 8.9364e-05 - mae: 0.0074\n",
      "Epoch 00015: early stopping\n",
      "2019SS =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 1s 29ms/sample - loss: 0.4304 - mae: 0.3032\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.5176e-04 - mae: 0.0151\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.6513e-04 - mae: 0.0088\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.5377e-04 - mae: 0.0091\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4133e-04 - mae: 0.0083\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4306e-04 - mae: 0.0097\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.3201e-04 - mae: 0.0086\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.1776e-04 - mae: 0.0081\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.9398e-04 - mae: 0.0108\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 1.4931e-04 - mae: 0.0092\n",
      "Epoch 00010: early stopping\n",
      "2019OB =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               77200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 77,301\n",
      "Trainable params: 77,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 2s 33ms/sample - loss: 0.3942 - mae: 0.2485\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 3.8743e-04 - mae: 0.0168\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.3896e-04 - mae: 0.0129\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.1795e-04 - mae: 0.0118\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.2692e-04 - mae: 0.0125\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 3ms/sample - loss: 2.5109e-04 - mae: 0.0130\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "model_dict = dict()\n",
    "hist_dict = dict()\n",
    "test_pred_df = pd.DataFrame([],columns = ['YEAR','T_ID','y','y_pred',\"shift_AVG_1\",\"shift_AVG_2\",'MSE','MSE_avg'])\n",
    "\n",
    "idx = 0\n",
    "for y in year:\n",
    "    tmp1 = lstm_train_X[lstm_train_X[\"YEAR\"] == y]\n",
    "    tmp2 = lstm_train_y[lstm_train_y[\"YEAR\"] == y]\n",
    "    tmp3 = lstm_test_X[lstm_test_X[\"YEAR\"] == y]\n",
    "    tmp4 = lstm_test_y[lstm_test_y[\"YEAR\"] == y]\n",
    "    for t in team:\n",
    "        name = '{}{}'.format(y,t)\n",
    "        print(name,\"=======================================\")\n",
    "        \n",
    "        X_train = tmp1[tmp1[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis = 1)\n",
    "        y_train = tmp2[tmp2[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis=1)\n",
    "        X_test = tmp3[tmp3[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis=1)\n",
    "        y_test = tmp4[tmp4[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis=1)\n",
    "        \n",
    "        X_train_v = X_train.values\n",
    "        y_train_v = y_train.values\n",
    "\n",
    "        X_test_v = X_test.values\n",
    "        y_test_v = y_test.values\n",
    "        \n",
    "        X_train_t = X_train_v.reshape(X_train_v.shape[0], 2,X_train_v.shape[1]//2)\n",
    "        X_test_t = X_test_v.reshape(X_test_v.shape[0], 2,X_test_v.shape[1]//2)\n",
    "        \n",
    "        ## model\n",
    "        K.clear_session() \n",
    "\n",
    "        model = Sequential()\n",
    "        optimizer = Adam(lr=0.01)\n",
    "#         optimizer = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "        model.add(LSTM(100,input_shape = (2,X_train_v.shape[1]//2))) # (timestep, feature)\n",
    "        model.add(Dense(1)) # output = 1\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer,metrics=['mae'])\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "#         hist1 = model.fit(X_train_t, y_train_v, epochs=100, batch_size=1, verbose=1)\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='loss', mode = 'min',patience=2, verbose=1)\n",
    "\n",
    "        hist1 = model.fit(X_train_t, y_train_v, epochs=100,\n",
    "                  batch_size=1, verbose=1, callbacks=[early_stop])\n",
    "        ##\n",
    "        \n",
    "        model_dict[name] = model\n",
    "        hist_dict[name] = hist1\n",
    "        \n",
    "        y_pred = model.predict(X_test_t)\n",
    "        mse = mean_squared_error(y_test_v, y_pred)\n",
    "        mse_avg = mean_squared_error(y_test_v,[y_train.mean()[0]])\n",
    "        \n",
    "        \n",
    "        test_pred_df.loc[idx,:] = [y,t,y_test_v.reshape(-1)[0],y_pred.reshape(-1)[0],\n",
    "                                  X_test.loc[X_test.index[0],[\"shift_AVG_1\"]][0],\n",
    "                                  X_test.loc[X_test.index[0],[\"shift_AVG_2\"]][0], mse,mse_avg]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "test_pred_df[['y','y_pred',\"shift_AVG_1\",\"shift_AVG_2\",'MSE','MSE_avg']] = test_pred_df[['y','y_pred',\"shift_AVG_1\",\"shift_AVG_2\",'MSE','MSE_avg']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005603615437343463"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test_pred_df['y'],test_pred_df['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_AVG_1</th>\n",
       "      <th>shift_AVG_2</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MSE_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.296069</td>\n",
       "      <td>0.297710</td>\n",
       "      <td>0.294471</td>\n",
       "      <td>0.297110</td>\n",
       "      <td>2.693360e-06</td>\n",
       "      <td>1.517778e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.288575</td>\n",
       "      <td>0.301862</td>\n",
       "      <td>0.293083</td>\n",
       "      <td>0.314581</td>\n",
       "      <td>1.765523e-04</td>\n",
       "      <td>2.450427e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.287440</td>\n",
       "      <td>0.291479</td>\n",
       "      <td>0.294611</td>\n",
       "      <td>0.286241</td>\n",
       "      <td>1.631802e-05</td>\n",
       "      <td>3.194852e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.256739</td>\n",
       "      <td>0.299684</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>0.313860</td>\n",
       "      <td>1.844245e-03</td>\n",
       "      <td>2.007821e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.300787</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>0.308046</td>\n",
       "      <td>2.003944e-05</td>\n",
       "      <td>1.099573e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.276185</td>\n",
       "      <td>0.281437</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>3.713209e-04</td>\n",
       "      <td>7.989715e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.289941</td>\n",
       "      <td>0.310660</td>\n",
       "      <td>0.297398</td>\n",
       "      <td>0.326291</td>\n",
       "      <td>4.292818e-04</td>\n",
       "      <td>3.572876e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.309893</td>\n",
       "      <td>0.275043</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>0.285211</td>\n",
       "      <td>1.214532e-03</td>\n",
       "      <td>8.075197e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.283863</td>\n",
       "      <td>0.316396</td>\n",
       "      <td>0.329186</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>1.058382e-03</td>\n",
       "      <td>4.781198e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.298225</td>\n",
       "      <td>0.293107</td>\n",
       "      <td>0.297974</td>\n",
       "      <td>0.285885</td>\n",
       "      <td>2.618811e-05</td>\n",
       "      <td>4.693716e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.274136</td>\n",
       "      <td>0.279718</td>\n",
       "      <td>0.251225</td>\n",
       "      <td>0.300725</td>\n",
       "      <td>3.116556e-05</td>\n",
       "      <td>3.676934e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.272944</td>\n",
       "      <td>0.296495</td>\n",
       "      <td>0.291463</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>5.546655e-04</td>\n",
       "      <td>6.486187e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.329682</td>\n",
       "      <td>0.300739</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>0.302850</td>\n",
       "      <td>8.377190e-04</td>\n",
       "      <td>1.085112e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.294464</td>\n",
       "      <td>0.245242</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.344456</td>\n",
       "      <td>2.422789e-03</td>\n",
       "      <td>8.718947e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.284160</td>\n",
       "      <td>0.271363</td>\n",
       "      <td>0.279268</td>\n",
       "      <td>0.275946</td>\n",
       "      <td>1.637475e-04</td>\n",
       "      <td>3.318524e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.303592</td>\n",
       "      <td>0.295594</td>\n",
       "      <td>0.292121</td>\n",
       "      <td>0.261635</td>\n",
       "      <td>6.397125e-05</td>\n",
       "      <td>8.032917e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.276888</td>\n",
       "      <td>0.301822</td>\n",
       "      <td>0.292108</td>\n",
       "      <td>0.283816</td>\n",
       "      <td>6.217096e-04</td>\n",
       "      <td>3.522284e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.297794</td>\n",
       "      <td>0.273520</td>\n",
       "      <td>0.286738</td>\n",
       "      <td>0.277051</td>\n",
       "      <td>5.892201e-04</td>\n",
       "      <td>4.417299e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.293588</td>\n",
       "      <td>0.278016</td>\n",
       "      <td>0.310658</td>\n",
       "      <td>0.266585</td>\n",
       "      <td>2.424928e-04</td>\n",
       "      <td>6.342718e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.288757</td>\n",
       "      <td>0.299996</td>\n",
       "      <td>0.289246</td>\n",
       "      <td>0.318025</td>\n",
       "      <td>1.263140e-04</td>\n",
       "      <td>3.743210e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.268675</td>\n",
       "      <td>0.274286</td>\n",
       "      <td>0.301038</td>\n",
       "      <td>0.289504</td>\n",
       "      <td>3.148538e-05</td>\n",
       "      <td>7.184501e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.286406</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>2.448489e-04</td>\n",
       "      <td>5.283648e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.283636</td>\n",
       "      <td>0.265834</td>\n",
       "      <td>0.276301</td>\n",
       "      <td>0.252888</td>\n",
       "      <td>3.169240e-04</td>\n",
       "      <td>4.459567e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.293083</td>\n",
       "      <td>0.306520</td>\n",
       "      <td>0.305065</td>\n",
       "      <td>0.286747</td>\n",
       "      <td>1.805470e-04</td>\n",
       "      <td>4.137268e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.275089</td>\n",
       "      <td>0.297724</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>5.123573e-04</td>\n",
       "      <td>6.720099e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.280925</td>\n",
       "      <td>0.256700</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.282660</td>\n",
       "      <td>5.868677e-04</td>\n",
       "      <td>1.408925e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.266908</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.334816</td>\n",
       "      <td>0.272093</td>\n",
       "      <td>4.408796e-08</td>\n",
       "      <td>5.650062e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.300691</td>\n",
       "      <td>0.289607</td>\n",
       "      <td>0.281287</td>\n",
       "      <td>0.282735</td>\n",
       "      <td>1.228528e-04</td>\n",
       "      <td>8.818226e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.295063</td>\n",
       "      <td>0.291092</td>\n",
       "      <td>0.291569</td>\n",
       "      <td>0.289598</td>\n",
       "      <td>1.577135e-05</td>\n",
       "      <td>1.080453e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.316629</td>\n",
       "      <td>0.341433</td>\n",
       "      <td>0.310142</td>\n",
       "      <td>0.327334</td>\n",
       "      <td>6.152305e-04</td>\n",
       "      <td>8.551172e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.258809</td>\n",
       "      <td>0.230018</td>\n",
       "      <td>0.254279</td>\n",
       "      <td>0.252207</td>\n",
       "      <td>8.289632e-04</td>\n",
       "      <td>6.411867e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.236341</td>\n",
       "      <td>0.277848</td>\n",
       "      <td>0.286219</td>\n",
       "      <td>0.260763</td>\n",
       "      <td>1.722855e-03</td>\n",
       "      <td>1.156787e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.255611</td>\n",
       "      <td>0.316463</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.317497</td>\n",
       "      <td>3.702958e-03</td>\n",
       "      <td>1.253596e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.264524</td>\n",
       "      <td>0.271643</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>5.068226e-05</td>\n",
       "      <td>5.108252e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.233207</td>\n",
       "      <td>0.266551</td>\n",
       "      <td>0.262893</td>\n",
       "      <td>0.275610</td>\n",
       "      <td>1.111873e-03</td>\n",
       "      <td>7.426226e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.265664</td>\n",
       "      <td>0.263613</td>\n",
       "      <td>0.268999</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>4.205240e-06</td>\n",
       "      <td>1.818321e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.260143</td>\n",
       "      <td>0.276017</td>\n",
       "      <td>0.292941</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>2.519637e-04</td>\n",
       "      <td>3.163669e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.304392</td>\n",
       "      <td>0.287286</td>\n",
       "      <td>0.268949</td>\n",
       "      <td>1.276990e-03</td>\n",
       "      <td>5.378564e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.279376</td>\n",
       "      <td>0.274539</td>\n",
       "      <td>0.238825</td>\n",
       "      <td>0.279597</td>\n",
       "      <td>2.340088e-05</td>\n",
       "      <td>8.889609e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.255786</td>\n",
       "      <td>0.255242</td>\n",
       "      <td>0.257822</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>2.951527e-07</td>\n",
       "      <td>1.819774e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred  shift_AVG_1  shift_AVG_2           MSE  \\\n",
       "0   2016   LG  0.296069  0.297710     0.294471     0.297110  2.693360e-06   \n",
       "1   2016   HH  0.288575  0.301862     0.293083     0.314581  1.765523e-04   \n",
       "2   2016   NC  0.287440  0.291479     0.294611     0.286241  1.631802e-05   \n",
       "3   2016   HT  0.256739  0.299684     0.293286     0.313860  1.844245e-03   \n",
       "4   2016   SK  0.305263  0.300787     0.286055     0.308046  2.003944e-05   \n",
       "5   2016   KT  0.295455  0.276185     0.281437     0.262626  3.713209e-04   \n",
       "6   2016   WO  0.289941  0.310660     0.297398     0.326291  4.292818e-04   \n",
       "7   2016   LT  0.309893  0.275043     0.256250     0.285211  1.214532e-03   \n",
       "8   2016   SS  0.283863  0.316396     0.329186     0.283688  1.058382e-03   \n",
       "9   2016   OB  0.298225  0.293107     0.297974     0.285885  2.618811e-05   \n",
       "10  2017   LG  0.274136  0.279718     0.251225     0.300725  3.116556e-05   \n",
       "11  2017   HH  0.272944  0.296495     0.291463     0.300000  5.546655e-04   \n",
       "12  2017   NC  0.329682  0.300739     0.261671     0.302850  8.377190e-04   \n",
       "13  2017   HT  0.294464  0.245242     0.292476     0.344456  2.422789e-03   \n",
       "14  2017   SK  0.284160  0.271363     0.279268     0.275946  1.637475e-04   \n",
       "15  2017   KT  0.303592  0.295594     0.292121     0.261635  6.397125e-05   \n",
       "16  2017   WO  0.276888  0.301822     0.292108     0.283816  6.217096e-04   \n",
       "17  2017   LT  0.297794  0.273520     0.286738     0.277051  5.892201e-04   \n",
       "18  2017   SS  0.293588  0.278016     0.310658     0.266585  2.424928e-04   \n",
       "19  2017   OB  0.288757  0.299996     0.289246     0.318025  1.263140e-04   \n",
       "20  2018   LG  0.268675  0.274286     0.301038     0.289504  3.148538e-05   \n",
       "21  2018   HH  0.270758  0.286406     0.286915     0.271186  2.448489e-04   \n",
       "22  2018   NC  0.283636  0.265834     0.276301     0.252888  3.169240e-04   \n",
       "23  2018   HT  0.293083  0.306520     0.305065     0.286747  1.805470e-04   \n",
       "24  2018   SK  0.275089  0.297724     0.283333     0.285185  5.123573e-04   \n",
       "25  2018   KT  0.280925  0.256700     0.262500     0.282660  5.868677e-04   \n",
       "26  2018   WO  0.266908  0.266698     0.334816     0.272093  4.408796e-08   \n",
       "27  2018   LT  0.300691  0.289607     0.281287     0.282735  1.228528e-04   \n",
       "28  2018   SS  0.295063  0.291092     0.291569     0.289598  1.577135e-05   \n",
       "29  2018   OB  0.316629  0.341433     0.310142     0.327334  6.152305e-04   \n",
       "30  2019   LG  0.258809  0.230018     0.254279     0.252207  8.289632e-04   \n",
       "31  2019   HH  0.236341  0.277848     0.286219     0.260763  1.722855e-03   \n",
       "32  2019   NC  0.255611  0.316463     0.275735     0.317497  3.702958e-03   \n",
       "33  2019   HT  0.264524  0.271643     0.265432     0.265133  5.068226e-05   \n",
       "34  2019   SK  0.233207  0.266551     0.262893     0.275610  1.111873e-03   \n",
       "35  2019   KT  0.265664  0.263613     0.268999     0.261538  4.205240e-06   \n",
       "36  2019   WO  0.260143  0.276017     0.292941     0.274533  2.519637e-04   \n",
       "37  2019   LT  0.268657  0.304392     0.287286     0.268949  1.276990e-03   \n",
       "38  2019   SS  0.279376  0.274539     0.238825     0.279597  2.340088e-05   \n",
       "39  2019   OB  0.255786  0.255242     0.257822     0.271845  2.951527e-07   \n",
       "\n",
       "         MSE_avg  \n",
       "0   1.517778e-05  \n",
       "1   2.450427e-04  \n",
       "2   3.194852e-07  \n",
       "3   2.007821e-03  \n",
       "4   1.099573e-05  \n",
       "5   7.989715e-04  \n",
       "6   3.572876e-04  \n",
       "7   8.075197e-04  \n",
       "8   4.781198e-05  \n",
       "9   4.693716e-05  \n",
       "10  3.676934e-04  \n",
       "11  6.486187e-04  \n",
       "12  1.085112e-03  \n",
       "13  8.718947e-04  \n",
       "14  3.318524e-04  \n",
       "15  8.032917e-04  \n",
       "16  3.522284e-04  \n",
       "17  4.417299e-04  \n",
       "18  6.342718e-05  \n",
       "19  3.743210e-04  \n",
       "20  7.184501e-04  \n",
       "21  5.283648e-05  \n",
       "22  4.459567e-04  \n",
       "23  4.137268e-05  \n",
       "24  6.720099e-05  \n",
       "25  1.408925e-04  \n",
       "26  5.650062e-04  \n",
       "27  8.818226e-05  \n",
       "28  1.080453e-05  \n",
       "29  8.551172e-05  \n",
       "30  6.411867e-07  \n",
       "31  1.156787e-03  \n",
       "32  1.253596e-03  \n",
       "33  5.108252e-05  \n",
       "34  7.426226e-04  \n",
       "35  1.818321e-04  \n",
       "36  3.163669e-04  \n",
       "37  5.378564e-05  \n",
       "38  8.889609e-05  \n",
       "39  1.819774e-05  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pred_df.sort_values(by=[\"YEAR\",\"T_ID\"]).to_csv(\"AVG_t2.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 데이터 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCT_lstm_train_X = pd.read_csv(\"AVG_lstm_final_train_X_hit.csv\")\n",
    "# PCT_lstm_train_y = pd.read_csv(\"AVG_lstm_final_train_y_hit.csv\")\n",
    "\n",
    "# PCT_lstm_test_X = pd.read_csv(\"AVG_lstm_final_test_X_hit.csv\")\n",
    "# PCT_lstm_test_y = pd.read_csv(\"AVG_lstm_final_test_y_hit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_AVG_1</th>\n",
       "      <th>shift_AVG_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.291146</td>\n",
       "      <td>0.287639</td>\n",
       "      <td>0.292375</td>\n",
       "      <td>0.296354</td>\n",
       "      <td>0.021336</td>\n",
       "      <td>0.015759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.283259</td>\n",
       "      <td>0.284698</td>\n",
       "      <td>0.293109</td>\n",
       "      <td>0.021382</td>\n",
       "      <td>0.022077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.284798</td>\n",
       "      <td>0.293297</td>\n",
       "      <td>0.283993</td>\n",
       "      <td>0.015053</td>\n",
       "      <td>0.012738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.275120</td>\n",
       "      <td>0.269043</td>\n",
       "      <td>0.272767</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.015692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y    y_pred  shift_AVG_1  shift_AVG_2       rms   rms_avg\n",
       "YEAR                                                                  \n",
       "2016  0.291146  0.287639     0.292375     0.296354  0.021336  0.015759\n",
       "2017  0.291601  0.283259     0.284698     0.293109  0.021382  0.022077\n",
       "2018  0.285146  0.284798     0.293297     0.283993  0.015053  0.012738\n",
       "2019  0.257812  0.275120     0.269043     0.272767  0.019414  0.015692"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df.groupby([\"YEAR\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_AVG_1</th>\n",
       "      <th>shift_AVG_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.296069</td>\n",
       "      <td>0.284477</td>\n",
       "      <td>0.294471</td>\n",
       "      <td>0.297110</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.288575</td>\n",
       "      <td>0.316201</td>\n",
       "      <td>0.293083</td>\n",
       "      <td>0.314581</td>\n",
       "      <td>0.027626</td>\n",
       "      <td>0.015654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.287440</td>\n",
       "      <td>0.270977</td>\n",
       "      <td>0.294611</td>\n",
       "      <td>0.286241</td>\n",
       "      <td>0.016463</td>\n",
       "      <td>0.000565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.256739</td>\n",
       "      <td>0.315041</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>0.313860</td>\n",
       "      <td>0.058302</td>\n",
       "      <td>0.044809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.299972</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>0.308046</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.003316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.268301</td>\n",
       "      <td>0.281437</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.027153</td>\n",
       "      <td>0.028266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.289941</td>\n",
       "      <td>0.293155</td>\n",
       "      <td>0.297398</td>\n",
       "      <td>0.326291</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.018902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.309893</td>\n",
       "      <td>0.265357</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>0.285211</td>\n",
       "      <td>0.044536</td>\n",
       "      <td>0.028417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.283863</td>\n",
       "      <td>0.276533</td>\n",
       "      <td>0.329186</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.007330</td>\n",
       "      <td>0.006915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.298225</td>\n",
       "      <td>0.286373</td>\n",
       "      <td>0.297974</td>\n",
       "      <td>0.285885</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.006851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.274136</td>\n",
       "      <td>0.277432</td>\n",
       "      <td>0.251225</td>\n",
       "      <td>0.300725</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.019175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.272944</td>\n",
       "      <td>0.290581</td>\n",
       "      <td>0.291463</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.017637</td>\n",
       "      <td>0.025468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.329682</td>\n",
       "      <td>0.268032</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>0.302850</td>\n",
       "      <td>0.061650</td>\n",
       "      <td>0.032941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.294464</td>\n",
       "      <td>0.300874</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.344456</td>\n",
       "      <td>0.006410</td>\n",
       "      <td>0.029528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.284160</td>\n",
       "      <td>0.264310</td>\n",
       "      <td>0.279268</td>\n",
       "      <td>0.275946</td>\n",
       "      <td>0.019850</td>\n",
       "      <td>0.018217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.303592</td>\n",
       "      <td>0.263965</td>\n",
       "      <td>0.292121</td>\n",
       "      <td>0.261635</td>\n",
       "      <td>0.039627</td>\n",
       "      <td>0.028342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.276888</td>\n",
       "      <td>0.297396</td>\n",
       "      <td>0.292108</td>\n",
       "      <td>0.283816</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.018768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.297794</td>\n",
       "      <td>0.272680</td>\n",
       "      <td>0.286738</td>\n",
       "      <td>0.277051</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.021017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.293588</td>\n",
       "      <td>0.291216</td>\n",
       "      <td>0.310658</td>\n",
       "      <td>0.266585</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.007964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.288757</td>\n",
       "      <td>0.306109</td>\n",
       "      <td>0.289246</td>\n",
       "      <td>0.318025</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.019347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.268675</td>\n",
       "      <td>0.313141</td>\n",
       "      <td>0.301038</td>\n",
       "      <td>0.289504</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.026804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.268664</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.007269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.283636</td>\n",
       "      <td>0.273120</td>\n",
       "      <td>0.276301</td>\n",
       "      <td>0.252888</td>\n",
       "      <td>0.010516</td>\n",
       "      <td>0.021118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.293083</td>\n",
       "      <td>0.277275</td>\n",
       "      <td>0.305065</td>\n",
       "      <td>0.286747</td>\n",
       "      <td>0.015808</td>\n",
       "      <td>0.006432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.275089</td>\n",
       "      <td>0.296218</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>0.021129</td>\n",
       "      <td>0.008198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.280925</td>\n",
       "      <td>0.256809</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.282660</td>\n",
       "      <td>0.024116</td>\n",
       "      <td>0.011870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.266908</td>\n",
       "      <td>0.260595</td>\n",
       "      <td>0.334816</td>\n",
       "      <td>0.272093</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.023770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.300691</td>\n",
       "      <td>0.285985</td>\n",
       "      <td>0.281287</td>\n",
       "      <td>0.282735</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.009391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.295063</td>\n",
       "      <td>0.291615</td>\n",
       "      <td>0.291569</td>\n",
       "      <td>0.289598</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.316629</td>\n",
       "      <td>0.324558</td>\n",
       "      <td>0.310142</td>\n",
       "      <td>0.327334</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.258809</td>\n",
       "      <td>0.261041</td>\n",
       "      <td>0.254279</td>\n",
       "      <td>0.252207</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.236341</td>\n",
       "      <td>0.265525</td>\n",
       "      <td>0.286219</td>\n",
       "      <td>0.260763</td>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.034012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.255611</td>\n",
       "      <td>0.301850</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.317497</td>\n",
       "      <td>0.046239</td>\n",
       "      <td>0.035406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.264524</td>\n",
       "      <td>0.276189</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>0.011665</td>\n",
       "      <td>0.007147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.233207</td>\n",
       "      <td>0.275072</td>\n",
       "      <td>0.262893</td>\n",
       "      <td>0.275610</td>\n",
       "      <td>0.041866</td>\n",
       "      <td>0.027251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.265664</td>\n",
       "      <td>0.263461</td>\n",
       "      <td>0.268999</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.013485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.260143</td>\n",
       "      <td>0.280680</td>\n",
       "      <td>0.292941</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>0.020537</td>\n",
       "      <td>0.017787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.298266</td>\n",
       "      <td>0.287286</td>\n",
       "      <td>0.268949</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.007334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.279376</td>\n",
       "      <td>0.271048</td>\n",
       "      <td>0.238825</td>\n",
       "      <td>0.279597</td>\n",
       "      <td>0.008329</td>\n",
       "      <td>0.009428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.255786</td>\n",
       "      <td>0.258064</td>\n",
       "      <td>0.257822</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.004266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred  shift_AVG_1  shift_AVG_2       rms  \\\n",
       "0   2016   LG  0.296069  0.284477     0.294471     0.297110  0.011592   \n",
       "1   2016   HH  0.288575  0.316201     0.293083     0.314581  0.027626   \n",
       "2   2016   NC  0.287440  0.270977     0.294611     0.286241  0.016463   \n",
       "3   2016   HT  0.256739  0.315041     0.293286     0.313860  0.058302   \n",
       "4   2016   SK  0.305263  0.299972     0.286055     0.308046  0.005292   \n",
       "5   2016   KT  0.295455  0.268301     0.281437     0.262626  0.027153   \n",
       "6   2016   WO  0.289941  0.293155     0.297398     0.326291  0.003215   \n",
       "7   2016   LT  0.309893  0.265357     0.256250     0.285211  0.044536   \n",
       "8   2016   SS  0.283863  0.276533     0.329186     0.283688  0.007330   \n",
       "9   2016   OB  0.298225  0.286373     0.297974     0.285885  0.011852   \n",
       "10  2017   LG  0.274136  0.277432     0.251225     0.300725  0.003296   \n",
       "11  2017   HH  0.272944  0.290581     0.291463     0.300000  0.017637   \n",
       "12  2017   NC  0.329682  0.268032     0.261671     0.302850  0.061650   \n",
       "13  2017   HT  0.294464  0.300874     0.292476     0.344456  0.006410   \n",
       "14  2017   SK  0.284160  0.264310     0.279268     0.275946  0.019850   \n",
       "15  2017   KT  0.303592  0.263965     0.292121     0.261635  0.039627   \n",
       "16  2017   WO  0.276888  0.297396     0.292108     0.283816  0.020508   \n",
       "17  2017   LT  0.297794  0.272680     0.286738     0.277051  0.025114   \n",
       "18  2017   SS  0.293588  0.291216     0.310658     0.266585  0.002373   \n",
       "19  2017   OB  0.288757  0.306109     0.289246     0.318025  0.017352   \n",
       "20  2018   LG  0.268675  0.313141     0.301038     0.289504  0.044466   \n",
       "21  2018   HH  0.270758  0.268664     0.286915     0.271186  0.002094   \n",
       "22  2018   NC  0.283636  0.273120     0.276301     0.252888  0.010516   \n",
       "23  2018   HT  0.293083  0.277275     0.305065     0.286747  0.015808   \n",
       "24  2018   SK  0.275089  0.296218     0.283333     0.285185  0.021129   \n",
       "25  2018   KT  0.280925  0.256809     0.262500     0.282660  0.024116   \n",
       "26  2018   WO  0.266908  0.260595     0.334816     0.272093  0.006314   \n",
       "27  2018   LT  0.300691  0.285985     0.281287     0.282735  0.014706   \n",
       "28  2018   SS  0.295063  0.291615     0.291569     0.289598  0.003449   \n",
       "29  2018   OB  0.316629  0.324558     0.310142     0.327334  0.007929   \n",
       "30  2019   LG  0.258809  0.261041     0.254279     0.252207  0.002232   \n",
       "31  2019   HH  0.236341  0.265525     0.286219     0.260763  0.029184   \n",
       "32  2019   NC  0.255611  0.301850     0.275735     0.317497  0.046239   \n",
       "33  2019   HT  0.264524  0.276189     0.265432     0.265133  0.011665   \n",
       "34  2019   SK  0.233207  0.275072     0.262893     0.275610  0.041866   \n",
       "35  2019   KT  0.265664  0.263461     0.268999     0.261538  0.002203   \n",
       "36  2019   WO  0.260143  0.280680     0.292941     0.274533  0.020537   \n",
       "37  2019   LT  0.268657  0.298266     0.287286     0.268949  0.029609   \n",
       "38  2019   SS  0.279376  0.271048     0.238825     0.279597  0.008329   \n",
       "39  2019   OB  0.255786  0.258064     0.257822     0.271845  0.002279   \n",
       "\n",
       "     rms_avg  \n",
       "0   0.003896  \n",
       "1   0.015654  \n",
       "2   0.000565  \n",
       "3   0.044809  \n",
       "4   0.003316  \n",
       "5   0.028266  \n",
       "6   0.018902  \n",
       "7   0.028417  \n",
       "8   0.006915  \n",
       "9   0.006851  \n",
       "10  0.019175  \n",
       "11  0.025468  \n",
       "12  0.032941  \n",
       "13  0.029528  \n",
       "14  0.018217  \n",
       "15  0.028342  \n",
       "16  0.018768  \n",
       "17  0.021017  \n",
       "18  0.007964  \n",
       "19  0.019347  \n",
       "20  0.026804  \n",
       "21  0.007269  \n",
       "22  0.021118  \n",
       "23  0.006432  \n",
       "24  0.008198  \n",
       "25  0.011870  \n",
       "26  0.023770  \n",
       "27  0.009391  \n",
       "28  0.003287  \n",
       "29  0.009247  \n",
       "30  0.000801  \n",
       "31  0.034012  \n",
       "32  0.035406  \n",
       "33  0.007147  \n",
       "34  0.027251  \n",
       "35  0.013485  \n",
       "36  0.017787  \n",
       "37  0.007334  \n",
       "38  0.009428  \n",
       "39  0.004266  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3438337229787254"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "tmp = test_pred_df.copy()\n",
    "# tmp['half']= 0.5\n",
    "r2_y_predict = r2_score(tmp['y'], tmp['y_pred'])\n",
    "r2_y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 아래 파일 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCT_lstm_train_X = pd.read_csv(\"AVG_lstm_final_train_X.csv\")\n",
    "# PCT_lstm_train_y = pd.read_csv(\"AVG_lstm_final_train_y.csv\")\n",
    "\n",
    "# PCT_lstm_test_X = pd.read_csv(\"AVG_lstm_final_test_X.csv\")\n",
    "# PCT_lstm_test_y = pd.read_csv(\"AVG_lstm_final_test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_AVG_1</th>\n",
       "      <th>shift_AVG_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.291146</td>\n",
       "      <td>0.292641</td>\n",
       "      <td>0.292375</td>\n",
       "      <td>0.296354</td>\n",
       "      <td>0.018380</td>\n",
       "      <td>0.015759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.291601</td>\n",
       "      <td>0.290575</td>\n",
       "      <td>0.284698</td>\n",
       "      <td>0.293109</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.022077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.285146</td>\n",
       "      <td>0.285081</td>\n",
       "      <td>0.293297</td>\n",
       "      <td>0.283993</td>\n",
       "      <td>0.012775</td>\n",
       "      <td>0.012738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.257812</td>\n",
       "      <td>0.261197</td>\n",
       "      <td>0.269043</td>\n",
       "      <td>0.272767</td>\n",
       "      <td>0.016623</td>\n",
       "      <td>0.015692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y    y_pred  shift_AVG_1  shift_AVG_2       rms   rms_avg\n",
       "YEAR                                                                  \n",
       "2016  0.291146  0.292641     0.292375     0.296354  0.018380  0.015759\n",
       "2017  0.291601  0.290575     0.284698     0.293109  0.020686  0.022077\n",
       "2018  0.285146  0.285081     0.293297     0.283993  0.012775  0.012738\n",
       "2019  0.257812  0.261197     0.269043     0.272767  0.016623  0.015692"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df.groupby([\"YEAR\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_AVG_1</th>\n",
       "      <th>shift_AVG_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.296069</td>\n",
       "      <td>0.293986</td>\n",
       "      <td>0.294471</td>\n",
       "      <td>0.297110</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.288575</td>\n",
       "      <td>0.310350</td>\n",
       "      <td>0.293083</td>\n",
       "      <td>0.314581</td>\n",
       "      <td>0.021776</td>\n",
       "      <td>0.015654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.287440</td>\n",
       "      <td>0.311469</td>\n",
       "      <td>0.294611</td>\n",
       "      <td>0.286241</td>\n",
       "      <td>0.024029</td>\n",
       "      <td>0.000565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.256739</td>\n",
       "      <td>0.299722</td>\n",
       "      <td>0.293286</td>\n",
       "      <td>0.313860</td>\n",
       "      <td>0.042983</td>\n",
       "      <td>0.044809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.305263</td>\n",
       "      <td>0.296655</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>0.308046</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>0.003316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.266835</td>\n",
       "      <td>0.281437</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.028620</td>\n",
       "      <td>0.028266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.289941</td>\n",
       "      <td>0.293779</td>\n",
       "      <td>0.297398</td>\n",
       "      <td>0.326291</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.018902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.309893</td>\n",
       "      <td>0.273169</td>\n",
       "      <td>0.256250</td>\n",
       "      <td>0.285211</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.028417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.283863</td>\n",
       "      <td>0.290610</td>\n",
       "      <td>0.329186</td>\n",
       "      <td>0.283688</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.006915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.298225</td>\n",
       "      <td>0.289828</td>\n",
       "      <td>0.297974</td>\n",
       "      <td>0.285885</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.006851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.274136</td>\n",
       "      <td>0.310476</td>\n",
       "      <td>0.251225</td>\n",
       "      <td>0.300725</td>\n",
       "      <td>0.036340</td>\n",
       "      <td>0.019175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.272944</td>\n",
       "      <td>0.309531</td>\n",
       "      <td>0.291463</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.036587</td>\n",
       "      <td>0.025468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.329682</td>\n",
       "      <td>0.298683</td>\n",
       "      <td>0.261671</td>\n",
       "      <td>0.302850</td>\n",
       "      <td>0.030999</td>\n",
       "      <td>0.032941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.294464</td>\n",
       "      <td>0.275053</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.344456</td>\n",
       "      <td>0.019411</td>\n",
       "      <td>0.029528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.284160</td>\n",
       "      <td>0.260247</td>\n",
       "      <td>0.279268</td>\n",
       "      <td>0.275946</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>0.018217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.303592</td>\n",
       "      <td>0.291121</td>\n",
       "      <td>0.292121</td>\n",
       "      <td>0.261635</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.028342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.276888</td>\n",
       "      <td>0.283369</td>\n",
       "      <td>0.292108</td>\n",
       "      <td>0.283816</td>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.018768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.297794</td>\n",
       "      <td>0.281346</td>\n",
       "      <td>0.286738</td>\n",
       "      <td>0.277051</td>\n",
       "      <td>0.016448</td>\n",
       "      <td>0.021017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.293588</td>\n",
       "      <td>0.288273</td>\n",
       "      <td>0.310658</td>\n",
       "      <td>0.266585</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>0.007964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.288757</td>\n",
       "      <td>0.307652</td>\n",
       "      <td>0.289246</td>\n",
       "      <td>0.318025</td>\n",
       "      <td>0.018894</td>\n",
       "      <td>0.019347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.268675</td>\n",
       "      <td>0.313541</td>\n",
       "      <td>0.301038</td>\n",
       "      <td>0.289504</td>\n",
       "      <td>0.044866</td>\n",
       "      <td>0.026804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.280807</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.010049</td>\n",
       "      <td>0.007269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.283636</td>\n",
       "      <td>0.264281</td>\n",
       "      <td>0.276301</td>\n",
       "      <td>0.252888</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.021118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.293083</td>\n",
       "      <td>0.294615</td>\n",
       "      <td>0.305065</td>\n",
       "      <td>0.286747</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.006432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.275089</td>\n",
       "      <td>0.270654</td>\n",
       "      <td>0.283333</td>\n",
       "      <td>0.285185</td>\n",
       "      <td>0.004434</td>\n",
       "      <td>0.008198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.280925</td>\n",
       "      <td>0.258651</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>0.282660</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>0.011870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.266908</td>\n",
       "      <td>0.269440</td>\n",
       "      <td>0.334816</td>\n",
       "      <td>0.272093</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.023770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.300691</td>\n",
       "      <td>0.286443</td>\n",
       "      <td>0.281287</td>\n",
       "      <td>0.282735</td>\n",
       "      <td>0.014248</td>\n",
       "      <td>0.009391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.295063</td>\n",
       "      <td>0.291178</td>\n",
       "      <td>0.291569</td>\n",
       "      <td>0.289598</td>\n",
       "      <td>0.003886</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.316629</td>\n",
       "      <td>0.321205</td>\n",
       "      <td>0.310142</td>\n",
       "      <td>0.327334</td>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.258809</td>\n",
       "      <td>0.236035</td>\n",
       "      <td>0.254279</td>\n",
       "      <td>0.252207</td>\n",
       "      <td>0.022774</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.236341</td>\n",
       "      <td>0.257743</td>\n",
       "      <td>0.286219</td>\n",
       "      <td>0.260763</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.034012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.255611</td>\n",
       "      <td>0.283232</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.317497</td>\n",
       "      <td>0.027621</td>\n",
       "      <td>0.035406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.264524</td>\n",
       "      <td>0.271931</td>\n",
       "      <td>0.265432</td>\n",
       "      <td>0.265133</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.233207</td>\n",
       "      <td>0.251132</td>\n",
       "      <td>0.262893</td>\n",
       "      <td>0.275610</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.027251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.265664</td>\n",
       "      <td>0.237244</td>\n",
       "      <td>0.268999</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.028420</td>\n",
       "      <td>0.013485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.260143</td>\n",
       "      <td>0.279308</td>\n",
       "      <td>0.292941</td>\n",
       "      <td>0.274533</td>\n",
       "      <td>0.019164</td>\n",
       "      <td>0.017787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.271239</td>\n",
       "      <td>0.287286</td>\n",
       "      <td>0.268949</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.007334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.279376</td>\n",
       "      <td>0.264379</td>\n",
       "      <td>0.238825</td>\n",
       "      <td>0.279597</td>\n",
       "      <td>0.014997</td>\n",
       "      <td>0.009428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.255786</td>\n",
       "      <td>0.259721</td>\n",
       "      <td>0.257822</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.004266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred  shift_AVG_1  shift_AVG_2       rms  \\\n",
       "0   2016   LG  0.296069  0.293986     0.294471     0.297110  0.002082   \n",
       "1   2016   HH  0.288575  0.310350     0.293083     0.314581  0.021776   \n",
       "2   2016   NC  0.287440  0.311469     0.294611     0.286241  0.024029   \n",
       "3   2016   HT  0.256739  0.299722     0.293286     0.313860  0.042983   \n",
       "4   2016   SK  0.305263  0.296655     0.286055     0.308046  0.008608   \n",
       "5   2016   KT  0.295455  0.266835     0.281437     0.262626  0.028620   \n",
       "6   2016   WO  0.289941  0.293779     0.297398     0.326291  0.003839   \n",
       "7   2016   LT  0.309893  0.273169     0.256250     0.285211  0.036723   \n",
       "8   2016   SS  0.283863  0.290610     0.329186     0.283688  0.006747   \n",
       "9   2016   OB  0.298225  0.289828     0.297974     0.285885  0.008397   \n",
       "10  2017   LG  0.274136  0.310476     0.251225     0.300725  0.036340   \n",
       "11  2017   HH  0.272944  0.309531     0.291463     0.300000  0.036587   \n",
       "12  2017   NC  0.329682  0.298683     0.261671     0.302850  0.030999   \n",
       "13  2017   HT  0.294464  0.275053     0.292476     0.344456  0.019411   \n",
       "14  2017   SK  0.284160  0.260247     0.279268     0.275946  0.023913   \n",
       "15  2017   KT  0.303592  0.291121     0.292121     0.261635  0.012471   \n",
       "16  2017   WO  0.276888  0.283369     0.292108     0.283816  0.006481   \n",
       "17  2017   LT  0.297794  0.281346     0.286738     0.277051  0.016448   \n",
       "18  2017   SS  0.293588  0.288273     0.310658     0.266585  0.005315   \n",
       "19  2017   OB  0.288757  0.307652     0.289246     0.318025  0.018894   \n",
       "20  2018   LG  0.268675  0.313541     0.301038     0.289504  0.044866   \n",
       "21  2018   HH  0.270758  0.280807     0.286915     0.271186  0.010049   \n",
       "22  2018   NC  0.283636  0.264281     0.276301     0.252888  0.019355   \n",
       "23  2018   HT  0.293083  0.294615     0.305065     0.286747  0.001532   \n",
       "24  2018   SK  0.275089  0.270654     0.283333     0.285185  0.004434   \n",
       "25  2018   KT  0.280925  0.258651     0.262500     0.282660  0.022274   \n",
       "26  2018   WO  0.266908  0.269440     0.334816     0.272093  0.002532   \n",
       "27  2018   LT  0.300691  0.286443     0.281287     0.282735  0.014248   \n",
       "28  2018   SS  0.295063  0.291178     0.291569     0.289598  0.003886   \n",
       "29  2018   OB  0.316629  0.321205     0.310142     0.327334  0.004577   \n",
       "30  2019   LG  0.258809  0.236035     0.254279     0.252207  0.022774   \n",
       "31  2019   HH  0.236341  0.257743     0.286219     0.260763  0.021403   \n",
       "32  2019   NC  0.255611  0.283232     0.275735     0.317497  0.027621   \n",
       "33  2019   HT  0.264524  0.271931     0.265432     0.265133  0.007407   \n",
       "34  2019   SK  0.233207  0.251132     0.262893     0.275610  0.017926   \n",
       "35  2019   KT  0.265664  0.237244     0.268999     0.261538  0.028420   \n",
       "36  2019   WO  0.260143  0.279308     0.292941     0.274533  0.019164   \n",
       "37  2019   LT  0.268657  0.271239     0.287286     0.268949  0.002582   \n",
       "38  2019   SS  0.279376  0.264379     0.238825     0.279597  0.014997   \n",
       "39  2019   OB  0.255786  0.259721     0.257822     0.271845  0.003935   \n",
       "\n",
       "     rms_avg  \n",
       "0   0.003896  \n",
       "1   0.015654  \n",
       "2   0.000565  \n",
       "3   0.044809  \n",
       "4   0.003316  \n",
       "5   0.028266  \n",
       "6   0.018902  \n",
       "7   0.028417  \n",
       "8   0.006915  \n",
       "9   0.006851  \n",
       "10  0.019175  \n",
       "11  0.025468  \n",
       "12  0.032941  \n",
       "13  0.029528  \n",
       "14  0.018217  \n",
       "15  0.028342  \n",
       "16  0.018768  \n",
       "17  0.021017  \n",
       "18  0.007964  \n",
       "19  0.019347  \n",
       "20  0.026804  \n",
       "21  0.007269  \n",
       "22  0.021118  \n",
       "23  0.006432  \n",
       "24  0.008198  \n",
       "25  0.011870  \n",
       "26  0.023770  \n",
       "27  0.009391  \n",
       "28  0.003287  \n",
       "29  0.009247  \n",
       "30  0.000801  \n",
       "31  0.034012  \n",
       "32  0.035406  \n",
       "33  0.007147  \n",
       "34  0.027251  \n",
       "35  0.013485  \n",
       "36  0.017787  \n",
       "37  0.007334  \n",
       "38  0.009428  \n",
       "39  0.004266  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
