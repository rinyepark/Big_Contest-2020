{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "import tensorflow.keras.backend as K \n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read data: augment_24group_1620.csv필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCT_lstm_train_X = pd.read_csv(\"lstmPCT/PCT_lstm_final_train_X.csv\")\n",
    "PCT_lstm_train_y = pd.read_csv(\"lstmPCT/PCT_lstm_final_train_y.csv\")\n",
    "\n",
    "PCT_lstm_test_X = pd.read_csv(\"lstmPCT/PCT_lstm_final_test_X.csv\")\n",
    "PCT_lstm_test_y = pd.read_csv(\"lstmPCT/PCT_lstm_final_test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T_ID</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LG</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LG</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LG</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LG</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LG</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>OB</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>OB</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>OB</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>OB</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>OB</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1960 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     T_ID  YEAR       PCT\n",
       "0      LG  2016  0.375000\n",
       "1      LG  2016  0.458333\n",
       "2      LG  2016  0.583333\n",
       "3      LG  2016  0.333333\n",
       "4      LG  2016  0.500000\n",
       "...   ...   ...       ...\n",
       "1955   OB  2019  0.583333\n",
       "1956   OB  2019  0.541667\n",
       "1957   OB  2019  0.583333\n",
       "1958   OB  2019  0.541667\n",
       "1959   OB  2019  0.625000\n",
       "\n",
       "[1960 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCT_lstm_train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = list(PCT_lstm_train_X.T_ID.unique())\n",
    "year = list(PCT_lstm_train_y.YEAR.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['LG', 'HH', 'NC', 'HT', 'SK', 'KT', 'WO', 'LT', 'SS', 'OB'],\n",
       " [2016, 2017, 2018, 2019])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team, year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) input shape로 변경 (row, timestep=2, feature)\n",
    "\n",
    "ex) \n",
    "timestep = 2\n",
    "\n",
    "* X_train_v 구성예시: [[1 ~ 24경기 데이터, 25 ~ 48경기 데이터], [49 ~ 72경기 데이터, 73 ~ 96경기 데이터] ]  \n",
    "X_train_v.shape >> (2,2*x)             # x: 각 24group에 대한 변수 개수\n",
    "* y_train_v 구성예시: 97 ~ 120 경기 승률\n",
    "\n",
    "=> reshape\n",
    "\n",
    "* X_train_v.shape >> (2,2,x)  # row, timestep, feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 구성(LSTM)\n",
    "- optimizer: RMSprop -> lr(learning rate) 조절\n",
    "- LSTM: 모델이 계속 동일한 결과값이 나올 때, input 뉴런 개수를 늘려야 한다는 글을 읽고 계속 input 노드 개수를 바꿔주면서 모델 생성중\n",
    "- loss: MSE\n",
    "\n",
    "- early_stop: patience를 크게하면 과적합 되는 경우가 있어서 최대한 작게 설정해둠\n",
    "- batch_size: 모델이 계속 동일한 결과값이 나올 때, 데이터가 적어 batch size를 줄여보라는 글을 읽고 1로 설정해둠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016LG =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               63200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 63,301\n",
      "Trainable params: 63,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 3s 68ms/sample - loss: 0.3131 - mae: 0.2610\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0225 - mae: 0.1398\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0214 - mae: 0.1223\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0197 - mae: 0.1247\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0149 - mae: 0.1034\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0047 - mae: 0.0525\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 6ms/sample - loss: 0.0028 - mae: 0.0417\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 6ms/sample - loss: 0.0038 - mae: 0.0506\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0017 - mae: 0.0321\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 9.9675e-04 - mae: 0.0254\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 9.3599e-04 - mae: 0.0236\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0013 - mae: 0.0305\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0019 - mae: 0.0371\n",
      "Epoch 00013: early stopping\n",
      "2016HH =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               63200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 63,301\n",
      "Trainable params: 63,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 3s 58ms/sample - loss: 0.3133 - mae: 0.2710\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0030 - mae: 0.0446\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0032 - mae: 0.0478\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0028 - mae: 0.0430\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0038 - mae: 0.0514\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0031 - mae: 0.0476\n",
      "Epoch 00006: early stopping\n",
      "2016NC =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               63200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 63,301\n",
      "Trainable params: 63,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      "49/49 [==============================] - 3s 60ms/sample - loss: 0.3163 - mae: 0.2876\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0036 - mae: 0.0472\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0034 - mae: 0.0424\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0037 - mae: 0.0450\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0028 - mae: 0.0402\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 0s 4ms/sample - loss: 0.0028 - mae: 0.0411\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 0s 5ms/sample - loss: 0.0036 - mae: 0.0458\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 0s 6ms/sample - loss: 0.0033 - mae: 0.0449\n",
      "Epoch 00008: early stopping\n",
      "2016HT =======================================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               63200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 63,301\n",
      "Trainable params: 63,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 49 samples\n",
      "Epoch 1/100\n",
      " 1/49 [..............................] - ETA: 46s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0927 22:16:39.978400  7780 callbacks.py:1250] Early stopping conditioned on metric `loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2382\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2384\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'while' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2386\u001b[0m       \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2387\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2388\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'while' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f990616ceef0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         hist1 = model.fit(X_train_t, y_train_v, epochs=100,\n\u001b[1;32m---> 47\u001b[1;33m                   batch_size=1, verbose=1, callbacks=[early_stop])\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;31m##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    501\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m       \u001b[0minitializer_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    406\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    407\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 408\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2150\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2151\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2040\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2041\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2042\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mdistributed_function\u001b[1;34m(input_iterator)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     outputs = strategy.experimental_run_v2(\n\u001b[1;32m---> 73\u001b[1;33m         per_replica_function, args=(model, x, y, sample_weights))\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Out of PerReplica outputs reduce or pick values to return.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     all_outputs = dist_utils.unwrap_output_dict(\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m       fn = autograph.tf_convert(fn, ag_ctx.control_status_ctx(),\n\u001b[0;32m    759\u001b[0m                                 convert_by_default=False)\n\u001b[1;32m--> 760\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   1785\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2130\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2131\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2132\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2134\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[0;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[0;32m    312\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[1;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[0;32m    266\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backwards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m           \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_total_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m           if isinstance(model.optimizer,\n\u001b[0;32m    270\u001b[0m                         loss_scale_optimizer.LossScaleOptimizer):\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_backward_function\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_backward_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m       \u001b[0mcall_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_backward_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_rewrite_forward_and_call_backward\u001b[1;34m(self, op, *doutputs)\u001b[0m\n\u001b[0;32m    659\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_rewrite_forward_and_call_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;34m\"\"\"Add outputs to the forward call and feed them to the grad function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m     \u001b[0mforward_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackwards_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbackwards_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mforward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_backward\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m     \u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_forward_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_function_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_doutputs\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_construct_forward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    627\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m           \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m           func_graph=backwards_graph)\n\u001b[0m\u001b[0;32m    630\u001b[0m       \u001b[0mbackwards_graph_captures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackwards_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexternal_captures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m       captures_from_forward = [\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    913\u001b[0m                                           converted_func)\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_backprop_function\u001b[1;34m(*grad_ys)\u001b[0m\n\u001b[0;32m    617\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m           \u001b[0mgrad_ys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 619\u001b[1;33m           src_graph=self._func_graph)\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py\u001b[0m in \u001b[0;36m_WhileGrad\u001b[1;34m(op, *grads)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m   \u001b[1;31m# See comment in while_loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_get_structured_grad_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody_grad_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\while_v2.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m   \u001b[1;31m# See comment in while_loop.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m   \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_get_structured_grad_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody_grad_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    212\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcopied\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m     \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4236\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4237\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 4238\u001b[1;33m         \"Identity\", input=input, name=name)\n\u001b[0m\u001b[0;32m   4239\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4240\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[0;32m    792\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 793\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    794\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    546\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    547\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3427\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3428\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3429\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3430\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3431\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1771\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1772\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1773\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1774\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1585\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1586\u001b[0m   op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n\u001b[1;32m-> 1587\u001b[1;33m                                   compat.as_str(node_def.name))\n\u001b[0m\u001b[0;32m   1588\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_dict = dict()\n",
    "hist_dict = dict()\n",
    "test_pred_df = pd.DataFrame([],columns = ['YEAR','T_ID','y','y_pred',\"shift_PCT_1\",\"shift_PCT_2\",'rms','rms_avg'])\n",
    "\n",
    "idx = 0\n",
    "for y in year:\n",
    "    tmp1 = PCT_lstm_train_X[PCT_lstm_train_X[\"YEAR\"] == y]\n",
    "    tmp2 = PCT_lstm_train_y[PCT_lstm_train_y[\"YEAR\"] == y]\n",
    "    tmp3 = PCT_lstm_test_X[PCT_lstm_test_X[\"YEAR\"] == y]\n",
    "    tmp4 = PCT_lstm_test_y[PCT_lstm_test_y[\"YEAR\"] == y]\n",
    "    for t in team:\n",
    "        name = '{}{}'.format(y,t)\n",
    "        print(name,\"=======================================\")\n",
    "        \n",
    "        X_train = tmp1[tmp1[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis = 1)\n",
    "        y_train = tmp2[tmp2[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis=1)\n",
    "        X_test = tmp3[tmp3[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis=1)\n",
    "        y_test = tmp4[tmp4[\"T_ID\"] == t].drop([\"T_ID\",\"YEAR\"],axis=1)\n",
    "        \n",
    "        X_train_v = X_train.values\n",
    "        y_train_v = y_train.values\n",
    "\n",
    "        X_test_v = X_test.values\n",
    "        y_test_v = y_test.values\n",
    "        \n",
    "        X_train_t = X_train_v.reshape(X_train_v.shape[0], 2,X_train_v.shape[1]//2)\n",
    "        X_test_t = X_test_v.reshape(X_test_v.shape[0], 2,X_test_v.shape[1]//2)\n",
    "        \n",
    "        ## model\n",
    "        K.clear_session() \n",
    "\n",
    "        model = Sequential()\n",
    "        optimizer = Adam(lr=0.01)\n",
    "#         optimizer = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "        model.add(LSTM(100,input_shape = (2,X_train_v.shape[1]//2))) # (timestep, feature)\n",
    "        model.add(Dense(1)) # output = 1\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizer,metrics=['mae'])\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "#         hist1 = model.fit(X_train_t, y_train_v, epochs=100, batch_size=1, verbose=1)\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='loss', mode = 'min',patience=2, verbose=1)\n",
    "\n",
    "        hist1 = model.fit(X_train_t, y_train_v, epochs=100,\n",
    "                  batch_size=1, verbose=1, callbacks=[early_stop])\n",
    "        ##\n",
    "        \n",
    "        model_dict[name] = model\n",
    "        hist_dict[name] = hist1\n",
    "        \n",
    "        y_pred = model.predict(X_test_t)\n",
    "        rms = sqrt(mean_squared_error(y_test_v, y_pred))\n",
    "        rms_avg = sqrt(mean_squared_error(y_test_v,[y_train.mean()[0]]))\n",
    "        \n",
    "        \n",
    "        test_pred_df.loc[idx,:] = [y,t,y_test_v.reshape(-1)[0],y_pred.reshape(-1)[0],\n",
    "                                  X_test.loc[X_test.index[0],[\"shift_PCT_1\"]][0],\n",
    "                                  X_test.loc[X_test.index[0],[\"shift_PCT_2\"]][0], rms,rms_avg]\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "test_pred_df[['y','y_pred',\"shift_PCT_1\",\"shift_PCT_2\",'rms','rms_avg']] = test_pred_df[['y','y_pred',\"shift_PCT_1\",\"shift_PCT_2\",'rms','rms_avg']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df.sort_values(by=[\"YEAR\",\"T_ID\"]).to_csv(\"PCT_t2.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013307317915064343"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(test_pred_df['y'],test_pred_df['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\julia\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\linalg\\linalg.py:1974: RuntimeWarning: invalid value encountered in greater\n",
      "  large = s > cutoff\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = test_pred_df.copy()\n",
    "sns.jointplot(x='y',y='y_pred', data=tmp, kind='reg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms0.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.492391</td>\n",
       "      <td>0.494809</td>\n",
       "      <td>0.504167</td>\n",
       "      <td>0.508514</td>\n",
       "      <td>0.072740</td>\n",
       "      <td>0.075725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.498370</td>\n",
       "      <td>0.488187</td>\n",
       "      <td>0.491848</td>\n",
       "      <td>0.521937</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.076630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.507971</td>\n",
       "      <td>0.517371</td>\n",
       "      <td>0.500362</td>\n",
       "      <td>0.497480</td>\n",
       "      <td>0.061284</td>\n",
       "      <td>0.071377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.505072</td>\n",
       "      <td>0.509891</td>\n",
       "      <td>0.508152</td>\n",
       "      <td>0.495290</td>\n",
       "      <td>0.123604</td>\n",
       "      <td>0.105072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y    y_pred  shift_PCT_1  shift_PCT_2       rms    rms0.5\n",
       "YEAR                                                                  \n",
       "2016  0.492391  0.494809     0.504167     0.508514  0.072740  0.075725\n",
       "2017  0.498370  0.488187     0.491848     0.521937  0.105915  0.076630\n",
       "2018  0.507971  0.517371     0.500362     0.497480  0.061284  0.071377\n",
       "2019  0.505072  0.509891     0.508152     0.495290  0.123604  0.105072"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df.groupby([\"YEAR\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016LG_y_pred</td>\n",
       "      <td>0.620275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016LG_rms</td>\n",
       "      <td>0.018753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016HH_y_pred</td>\n",
       "      <td>0.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016HH_rms</td>\n",
       "      <td>0.032717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016NC_y_pred</td>\n",
       "      <td>0.521020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016NC_rms</td>\n",
       "      <td>0.044198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016HT_y_pred</td>\n",
       "      <td>0.569602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016HT_rms</td>\n",
       "      <td>0.111269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016SK_y_pred</td>\n",
       "      <td>0.436350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016SK_rms</td>\n",
       "      <td>0.022064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016KT_y_pred</td>\n",
       "      <td>0.315432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016KT_rms</td>\n",
       "      <td>0.024908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016WO_y_pred</td>\n",
       "      <td>0.575864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016WO_rms</td>\n",
       "      <td>0.200864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2016LT_y_pred</td>\n",
       "      <td>0.395361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016LT_rms</td>\n",
       "      <td>0.104639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016SS_y_pred</td>\n",
       "      <td>0.496255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2016SS_rms</td>\n",
       "      <td>0.012541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2016OB_y_pred</td>\n",
       "      <td>0.520508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016OB_rms</td>\n",
       "      <td>0.146159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            index         0\n",
       "0   2016LG_y_pred  0.620275\n",
       "1      2016LG_rms  0.018753\n",
       "2   2016HH_y_pred  0.529412\n",
       "3      2016HH_rms  0.032717\n",
       "4   2016NC_y_pred  0.521020\n",
       "5      2016NC_rms  0.044198\n",
       "6   2016HT_y_pred  0.569602\n",
       "7      2016HT_rms  0.111269\n",
       "8   2016SK_y_pred  0.436350\n",
       "9      2016SK_rms  0.022064\n",
       "10  2016KT_y_pred  0.315432\n",
       "11     2016KT_rms  0.024908\n",
       "12  2016WO_y_pred  0.575864\n",
       "13     2016WO_rms  0.200864\n",
       "14  2016LT_y_pred  0.395361\n",
       "15     2016LT_rms  0.104639\n",
       "16  2016SS_y_pred  0.496255\n",
       "17     2016SS_rms  0.012541\n",
       "18  2016OB_y_pred  0.520508\n",
       "19     2016OB_rms  0.146159"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddd.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.515102</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.093594</td>\n",
       "      <td>0.126553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.553883</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.053883</td>\n",
       "      <td>0.031221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.532066</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.033152</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.545200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.086866</td>\n",
       "      <td>0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.413113</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.045220</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.316462</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.024795</td>\n",
       "      <td>0.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.563730</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.188730</td>\n",
       "      <td>0.231293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.464201</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.035799</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.380382</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.119618</td>\n",
       "      <td>0.080708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.595040</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.071627</td>\n",
       "      <td>0.128401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.534067</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.099285</td>\n",
       "      <td>0.076050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.382721</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.052062</td>\n",
       "      <td>0.017746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.503140</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.038265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.481882</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>0.118123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.519574</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.105426</td>\n",
       "      <td>0.151361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.239496</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.279762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.554023</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.206197</td>\n",
       "      <td>0.205745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.578560</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.088107</td>\n",
       "      <td>0.104431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.457924</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.066619</td>\n",
       "      <td>0.082628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.694552</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.111218</td>\n",
       "      <td>0.055753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.447363</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>0.043959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.547442</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.047442</td>\n",
       "      <td>0.036565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.442730</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.026063</td>\n",
       "      <td>0.049357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.400716</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.140951</td>\n",
       "      <td>0.115646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.529142</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.070809</td>\n",
       "      <td>0.105812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.376548</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.029984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.455911</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>0.022959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.431132</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.235535</td>\n",
       "      <td>0.206223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.485655</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.036084</td>\n",
       "      <td>0.013794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.615613</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.009387</td>\n",
       "      <td>0.003401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.467227</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.074440</td>\n",
       "      <td>0.000481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.254333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.388191</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.177026</td>\n",
       "      <td>0.124630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.471465</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>0.029429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.729794</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.354794</td>\n",
       "      <td>0.297619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.573910</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.032244</td>\n",
       "      <td>0.033126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.654181</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.012486</td>\n",
       "      <td>0.004621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.427392</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.219058</td>\n",
       "      <td>0.187512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.458591</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.041925</td>\n",
       "      <td>0.006914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.569024</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.083150</td>\n",
       "      <td>0.117310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred  shift_PCT_1  shift_PCT_2       rms  \\\n",
       "0   2016   LG  0.608696  0.515102     0.583333     0.458333  0.093594   \n",
       "1   2016   HH  0.500000  0.553883     0.500000     0.608696  0.053883   \n",
       "2   2016   NC  0.565217  0.532066     0.500000     0.541667  0.033152   \n",
       "3   2016   HT  0.458333  0.545200     0.500000     0.583333  0.086866   \n",
       "4   2016   SK  0.458333  0.413113     0.458333     0.458333  0.045220   \n",
       "5   2016   KT  0.291667  0.316462     0.333333     0.375000  0.024795   \n",
       "6   2016   WO  0.375000  0.563730     0.625000     0.666667  0.188730   \n",
       "7   2016   LT  0.500000  0.464201     0.375000     0.500000  0.035799   \n",
       "8   2016   SS  0.500000  0.380382     0.500000     0.434783  0.119618   \n",
       "9   2016   OB  0.666667  0.595040     0.666667     0.458333  0.071627   \n",
       "10  2017   LG  0.434783  0.534067     0.304348     0.652174  0.099285   \n",
       "11  2017   HH  0.434783  0.382721     0.541667     0.333333  0.052062   \n",
       "12  2017   NC  0.500000  0.503140     0.458333     0.541667  0.003140   \n",
       "13  2017   HT  0.500000  0.481882     0.500000     0.782609  0.018118   \n",
       "14  2017   SK  0.625000  0.519574     0.458333     0.416667  0.105426   \n",
       "15  2017   KT  0.500000  0.239496     0.333333     0.166667  0.260504   \n",
       "16  2017   WO  0.347826  0.554023     0.500000     0.541667  0.206197   \n",
       "17  2017   LT  0.666667  0.578560     0.750000     0.590909  0.088107   \n",
       "18  2017   SS  0.391304  0.457924     0.333333     0.454545  0.066619   \n",
       "19  2017   OB  0.583333  0.694552     0.739130     0.739130  0.111218   \n",
       "20  2018   LG  0.416667  0.447363     0.291667     0.478261  0.030696   \n",
       "21  2018   HH  0.500000  0.547442     0.416667     0.583333  0.047442   \n",
       "22  2018   NC  0.416667  0.442730     0.521739     0.416667  0.026063   \n",
       "23  2018   HT  0.541667  0.400716     0.541667     0.375000  0.140951   \n",
       "24  2018   SK  0.458333  0.529142     0.500000     0.666667  0.070809   \n",
       "25  2018   KT  0.391304  0.376548     0.416667     0.545455  0.014756   \n",
       "26  2018   WO  0.541667  0.455911     0.666667     0.458333  0.085756   \n",
       "27  2018   LT  0.666667  0.431132     0.458333     0.347826  0.235535   \n",
       "28  2018   SS  0.521739  0.485655     0.565217     0.478261  0.036084   \n",
       "29  2018   OB  0.625000  0.615613     0.625000     0.625000  0.009387   \n",
       "30  2019   LG  0.541667  0.467227     0.541667     0.500000  0.074440   \n",
       "31  2019   HH  0.583333  0.254333     0.375000     0.250000  0.329000   \n",
       "32  2019   NC  0.565217  0.388191     0.541667     0.478261  0.177026   \n",
       "33  2019   HT  0.500000  0.471465     0.478261     0.416667  0.028535   \n",
       "34  2019   SK  0.375000  0.729794     0.625000     0.708333  0.354794   \n",
       "35  2019   KT  0.541667  0.573910     0.521739     0.652174  0.032244   \n",
       "36  2019   WO  0.666667  0.654181     0.565217     0.708333  0.012486   \n",
       "37  2019   LT  0.208333  0.427392     0.391304     0.304348  0.219058   \n",
       "38  2019   SS  0.416667  0.458591     0.375000     0.434783  0.041925   \n",
       "39  2019   OB  0.652174  0.569024     0.666667     0.500000  0.083150   \n",
       "\n",
       "     rms_avg  \n",
       "0   0.126553  \n",
       "1   0.031221  \n",
       "2   0.017857  \n",
       "3   0.095238  \n",
       "4   0.040816  \n",
       "5   0.059524  \n",
       "6   0.231293  \n",
       "7   0.041667  \n",
       "8   0.080708  \n",
       "9   0.128401  \n",
       "10  0.076050  \n",
       "11  0.017746  \n",
       "12  0.038265  \n",
       "13  0.118123  \n",
       "14  0.151361  \n",
       "15  0.279762  \n",
       "16  0.205745  \n",
       "17  0.104431  \n",
       "18  0.082628  \n",
       "19  0.055753  \n",
       "20  0.043959  \n",
       "21  0.036565  \n",
       "22  0.049357  \n",
       "23  0.115646  \n",
       "24  0.105812  \n",
       "25  0.029984  \n",
       "26  0.022959  \n",
       "27  0.206223  \n",
       "28  0.013794  \n",
       "29  0.003401  \n",
       "30  0.000481  \n",
       "31  0.291667  \n",
       "32  0.124630  \n",
       "33  0.029429  \n",
       "34  0.297619  \n",
       "35  0.033126  \n",
       "36  0.004621  \n",
       "37  0.187512  \n",
       "38  0.006914  \n",
       "39  0.117310  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pred_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-10adfa98814c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_pred_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# tmp['half']= 0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mr2_y_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_pred'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr2_y_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pred_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "tmp = test_pred_df.copy()\n",
    "# tmp['half']= 0.5\n",
    "r2_y_predict = r2_score(tmp['y'], tmp['y_pred'])\n",
    "r2_y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +======================================+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.522544</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.086151</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510781</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.543209</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.577802</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.119468</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.522475</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.064141</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.328833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.037166</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.600633</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.225633</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.470777</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.029223</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.440369</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.537850</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.128817</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.461004</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.026221</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.379429</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.055354</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.498975</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.594801</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.094801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.505057</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.119943</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.206646</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.293354</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.577420</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.229594</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.491583</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.175083</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.492729</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.101425</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.566920</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.444207</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.027540</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.574532</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.074532</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.440569</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.023902</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.435152</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.106515</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.568434</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.110100</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.406778</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.015473</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.514372</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.027295</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.457512</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.209154</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.532034</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.021739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.641725</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.485760</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.055907</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.302395</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.280938</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.468876</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.096342</td>\n",
       "      <td>0.065217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.478172</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.711183</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.336183</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.565580</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.023914</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.684416</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.017749</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.423814</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.215481</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.441931</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.025264</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.102236</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred  shift_PCT_1  shift_PCT_2       rms  \\\n",
       "0   2016   LG  0.608696  0.522544     0.583333     0.458333  0.086151   \n",
       "1   2016   HH  0.500000  0.510781     0.500000     0.608696  0.010781   \n",
       "2   2016   NC  0.565217  0.543209     0.500000     0.541667  0.022008   \n",
       "3   2016   HT  0.458333  0.577802     0.500000     0.583333  0.119468   \n",
       "4   2016   SK  0.458333  0.522475     0.458333     0.458333  0.064141   \n",
       "5   2016   KT  0.291667  0.328833     0.333333     0.375000  0.037166   \n",
       "6   2016   WO  0.375000  0.600633     0.625000     0.666667  0.225633   \n",
       "7   2016   LT  0.500000  0.470777     0.375000     0.500000  0.029223   \n",
       "8   2016   SS  0.500000  0.440369     0.500000     0.434783  0.059631   \n",
       "9   2016   OB  0.666667  0.537850     0.666667     0.458333  0.128817   \n",
       "10  2017   LG  0.434783  0.461004     0.304348     0.652174  0.026221   \n",
       "11  2017   HH  0.434783  0.379429     0.541667     0.333333  0.055354   \n",
       "12  2017   NC  0.500000  0.498975     0.458333     0.541667  0.001025   \n",
       "13  2017   HT  0.500000  0.594801     0.500000     0.782609  0.094801   \n",
       "14  2017   SK  0.625000  0.505057     0.458333     0.416667  0.119943   \n",
       "15  2017   KT  0.500000  0.206646     0.333333     0.166667  0.293354   \n",
       "16  2017   WO  0.347826  0.577420     0.500000     0.541667  0.229594   \n",
       "17  2017   LT  0.666667  0.491583     0.750000     0.590909  0.175083   \n",
       "18  2017   SS  0.391304  0.492729     0.333333     0.454545  0.101425   \n",
       "19  2017   OB  0.583333  0.566920     0.739130     0.739130  0.016413   \n",
       "20  2018   LG  0.416667  0.444207     0.291667     0.478261  0.027540   \n",
       "21  2018   HH  0.500000  0.574532     0.416667     0.583333  0.074532   \n",
       "22  2018   NC  0.416667  0.440569     0.521739     0.416667  0.023902   \n",
       "23  2018   HT  0.541667  0.435152     0.541667     0.375000  0.106515   \n",
       "24  2018   SK  0.458333  0.568434     0.500000     0.666667  0.110100   \n",
       "25  2018   KT  0.391304  0.406778     0.416667     0.545455  0.015473   \n",
       "26  2018   WO  0.541667  0.514372     0.666667     0.458333  0.027295   \n",
       "27  2018   LT  0.666667  0.457512     0.458333     0.347826  0.209154   \n",
       "28  2018   SS  0.521739  0.532034     0.565217     0.478261  0.010295   \n",
       "29  2018   OB  0.625000  0.641725     0.625000     0.625000  0.016725   \n",
       "30  2019   LG  0.541667  0.485760     0.541667     0.500000  0.055907   \n",
       "31  2019   HH  0.583333  0.302395     0.375000     0.250000  0.280938   \n",
       "32  2019   NC  0.565217  0.468876     0.541667     0.478261  0.096342   \n",
       "33  2019   HT  0.500000  0.478172     0.478261     0.416667  0.021828   \n",
       "34  2019   SK  0.375000  0.711183     0.625000     0.708333  0.336183   \n",
       "35  2019   KT  0.541667  0.565580     0.521739     0.652174  0.023914   \n",
       "36  2019   WO  0.666667  0.684416     0.565217     0.708333  0.017749   \n",
       "37  2019   LT  0.208333  0.423814     0.391304     0.304348  0.215481   \n",
       "38  2019   SS  0.416667  0.441931     0.375000     0.434783  0.025264   \n",
       "39  2019   OB  0.652174  0.549938     0.666667     0.500000  0.102236   \n",
       "\n",
       "      rms0.5  \n",
       "0   0.108696  \n",
       "1   0.000000  \n",
       "2   0.065217  \n",
       "3   0.041667  \n",
       "4   0.041667  \n",
       "5   0.208333  \n",
       "6   0.125000  \n",
       "7   0.000000  \n",
       "8   0.000000  \n",
       "9   0.166667  \n",
       "10  0.065217  \n",
       "11  0.065217  \n",
       "12  0.000000  \n",
       "13  0.000000  \n",
       "14  0.125000  \n",
       "15  0.000000  \n",
       "16  0.152174  \n",
       "17  0.166667  \n",
       "18  0.108696  \n",
       "19  0.083333  \n",
       "20  0.083333  \n",
       "21  0.000000  \n",
       "22  0.083333  \n",
       "23  0.041667  \n",
       "24  0.041667  \n",
       "25  0.108696  \n",
       "26  0.041667  \n",
       "27  0.166667  \n",
       "28  0.021739  \n",
       "29  0.125000  \n",
       "30  0.041667  \n",
       "31  0.083333  \n",
       "32  0.065217  \n",
       "33  0.000000  \n",
       "34  0.125000  \n",
       "35  0.041667  \n",
       "36  0.166667  \n",
       "37  0.291667  \n",
       "38  0.083333  \n",
       "39  0.152174  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #100, lr = 0.01, patience 2 +++, rms ver, rms05 ver, Adam, batch2\n",
    "\n",
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms0.5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YEAR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0.492391</td>\n",
       "      <td>0.505527</td>\n",
       "      <td>0.504167</td>\n",
       "      <td>0.508514</td>\n",
       "      <td>0.078302</td>\n",
       "      <td>0.075725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0.498370</td>\n",
       "      <td>0.477456</td>\n",
       "      <td>0.491848</td>\n",
       "      <td>0.521937</td>\n",
       "      <td>0.111321</td>\n",
       "      <td>0.076630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0.507971</td>\n",
       "      <td>0.501531</td>\n",
       "      <td>0.500362</td>\n",
       "      <td>0.497480</td>\n",
       "      <td>0.062153</td>\n",
       "      <td>0.071377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>0.505072</td>\n",
       "      <td>0.511206</td>\n",
       "      <td>0.508152</td>\n",
       "      <td>0.495290</td>\n",
       "      <td>0.117584</td>\n",
       "      <td>0.105072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             y    y_pred  shift_PCT_1  shift_PCT_2       rms    rms0.5\n",
       "YEAR                                                                  \n",
       "2016  0.492391  0.505527     0.504167     0.508514  0.078302  0.075725\n",
       "2017  0.498370  0.477456     0.491848     0.521937  0.111321  0.076630\n",
       "2018  0.507971  0.501531     0.500362     0.497480  0.062153  0.071377\n",
       "2019  0.505072  0.511206     0.508152     0.495290  0.117584  0.105072"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df.groupby([\"YEAR\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.468182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.567216</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.492161</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.468182    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.567216         0.5    0.608696\n",
       "2  2016   NC  0.565217  0.492161         0.5    0.541667"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #300, lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.423421</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.479113</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.423421    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.479113         0.5    0.608696"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #300, lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.490298</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.672277</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.653289</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.490298    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.672277         0.5    0.608696\n",
       "2  2016   NC  0.565217  0.653289         0.5    0.541667"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #300, lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>2.45143</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-2.15025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>-1.5099</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y   y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  2.45143    0.583333    0.458333\n",
       "1  2016   HH       0.5 -2.15025         0.5    0.608696\n",
       "2  2016   NC  0.565217  -1.5099         0.5    0.541667"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.46766</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.554741</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.540454</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.586706</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.492073</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.450251</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.629395</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.417594</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.488712</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.553688</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.565867</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.31171</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.487845</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.464065</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.377638</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.204735</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0   2016   LG  0.608696   0.46766    0.583333    0.458333\n",
       "1   2016   HH       0.5  0.554741         0.5    0.608696\n",
       "2   2016   NC  0.565217  0.540454         0.5    0.541667\n",
       "3   2016   HT  0.458333  0.586706         0.5    0.583333\n",
       "4   2016   SK  0.458333  0.492073    0.458333    0.458333\n",
       "5   2016   KT  0.291667  0.450251    0.333333       0.375\n",
       "6   2016   WO     0.375  0.629395       0.625    0.666667\n",
       "7   2016   LT       0.5  0.417594       0.375         0.5\n",
       "8   2016   SS       0.5  0.488712         0.5    0.434783\n",
       "9   2016   OB  0.666667  0.553688    0.666667    0.458333\n",
       "10  2017   LG  0.434783  0.565867    0.304348    0.652174\n",
       "11  2017   HH  0.434783   0.31171    0.541667    0.333333\n",
       "12  2017   NC       0.5  0.487845    0.458333    0.541667\n",
       "13  2017   HT       0.5  0.464065         0.5    0.782609\n",
       "14  2017   SK     0.625  0.377638    0.458333    0.416667\n",
       "15  2017   KT       0.5  0.204735    0.333333    0.166667"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.01, patience 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.477605</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.509489</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.519098</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.585744</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.462029</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.333392</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.543434</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.377705</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.400355</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.539122</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.500583</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.435981</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.498783</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0   2016   LG  0.608696  0.477605    0.583333    0.458333\n",
       "1   2016   HH       0.5  0.509489         0.5    0.608696\n",
       "2   2016   NC  0.565217  0.519098         0.5    0.541667\n",
       "3   2016   HT  0.458333  0.585744         0.5    0.583333\n",
       "4   2016   SK  0.458333  0.462029    0.458333    0.458333\n",
       "5   2016   KT  0.291667  0.333392    0.333333       0.375\n",
       "6   2016   WO     0.375  0.543434       0.625    0.666667\n",
       "7   2016   LT       0.5  0.377705       0.375         0.5\n",
       "8   2016   SS       0.5  0.400355         0.5    0.434783\n",
       "9   2016   OB  0.666667  0.539122    0.666667    0.458333\n",
       "10  2017   LG  0.434783  0.500583    0.304348    0.652174\n",
       "11  2017   HH  0.434783  0.435981    0.541667    0.333333\n",
       "12  2017   NC       0.5  0.498783    0.458333    0.541667"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.01, patience 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.506664</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.102032</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.518261</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.0182612</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.520184</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.0450339</td>\n",
       "      <td>0.0652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.562111</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.103777</td>\n",
       "      <td>0.0416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.487501</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0291681</td>\n",
       "      <td>0.0416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.372884</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0812176</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.578908</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.203908</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.431082</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0689185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4259</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.0740999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.496237</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.170429</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.468442</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.0336597</td>\n",
       "      <td>0.0652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.455786</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0210039</td>\n",
       "      <td>0.0652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.531187</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.0311873</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2        rms  \\\n",
       "0   2016   LG  0.608696  0.506664    0.583333    0.458333   0.102032   \n",
       "1   2016   HH       0.5  0.518261         0.5    0.608696  0.0182612   \n",
       "2   2016   NC  0.565217  0.520184         0.5    0.541667  0.0450339   \n",
       "3   2016   HT  0.458333  0.562111         0.5    0.583333   0.103777   \n",
       "4   2016   SK  0.458333  0.487501    0.458333    0.458333  0.0291681   \n",
       "5   2016   KT  0.291667  0.372884    0.333333       0.375  0.0812176   \n",
       "6   2016   WO     0.375  0.578908       0.625    0.666667   0.203908   \n",
       "7   2016   LT       0.5  0.431082       0.375         0.5  0.0689185   \n",
       "8   2016   SS       0.5    0.4259         0.5    0.434783  0.0740999   \n",
       "9   2016   OB  0.666667  0.496237    0.666667    0.458333   0.170429   \n",
       "10  2017   LG  0.434783  0.468442    0.304348    0.652174  0.0336597   \n",
       "11  2017   HH  0.434783  0.455786    0.541667    0.333333  0.0210039   \n",
       "12  2017   NC       0.5  0.531187    0.458333    0.541667  0.0311873   \n",
       "\n",
       "       rms0.5  \n",
       "0    0.108696  \n",
       "1           0  \n",
       "2   0.0652174  \n",
       "3   0.0416667  \n",
       "4   0.0416667  \n",
       "5    0.208333  \n",
       "6       0.125  \n",
       "7           0  \n",
       "8           0  \n",
       "9    0.166667  \n",
       "10  0.0652174  \n",
       "11  0.0652174  \n",
       "12          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #100, lr = 0.01, patience 2 +++, rms ver, rms05 ver, Adam\n",
    "\n",
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "      <th>rms0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.612487</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.00379184</td>\n",
       "      <td>0.108696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434664</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.0653357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.529742</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.0354756</td>\n",
       "      <td>0.0652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.542623</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0842892</td>\n",
       "      <td>0.0416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.442084</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0162496</td>\n",
       "      <td>0.0416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.350306</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0586392</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.586355</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.211355</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.475668</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0243322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.475334</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.024666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.537623</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.129044</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.519807</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.0850246</td>\n",
       "      <td>0.0652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.367407</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0673759</td>\n",
       "      <td>0.0652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.489935</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.0100647</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.508418</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.00841796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.745366</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.120366</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.236669</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.263331</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2         rms  \\\n",
       "0   2016   LG  0.608696  0.612487    0.583333    0.458333  0.00379184   \n",
       "1   2016   HH       0.5  0.434664         0.5    0.608696   0.0653357   \n",
       "2   2016   NC  0.565217  0.529742         0.5    0.541667   0.0354756   \n",
       "3   2016   HT  0.458333  0.542623         0.5    0.583333   0.0842892   \n",
       "4   2016   SK  0.458333  0.442084    0.458333    0.458333   0.0162496   \n",
       "5   2016   KT  0.291667  0.350306    0.333333       0.375   0.0586392   \n",
       "6   2016   WO     0.375  0.586355       0.625    0.666667    0.211355   \n",
       "7   2016   LT       0.5  0.475668       0.375         0.5   0.0243322   \n",
       "8   2016   SS       0.5  0.475334         0.5    0.434783    0.024666   \n",
       "9   2016   OB  0.666667  0.537623    0.666667    0.458333    0.129044   \n",
       "10  2017   LG  0.434783  0.519807    0.304348    0.652174   0.0850246   \n",
       "11  2017   HH  0.434783  0.367407    0.541667    0.333333   0.0673759   \n",
       "12  2017   NC       0.5  0.489935    0.458333    0.541667   0.0100647   \n",
       "13  2017   HT       0.5  0.508418         0.5    0.782609  0.00841796   \n",
       "14  2017   SK     0.625  0.745366    0.458333    0.416667    0.120366   \n",
       "15  2017   KT       0.5  0.236669    0.333333    0.166667    0.263331   \n",
       "\n",
       "       rms0.5  \n",
       "0    0.108696  \n",
       "1           0  \n",
       "2   0.0652174  \n",
       "3   0.0416667  \n",
       "4   0.0416667  \n",
       "5    0.208333  \n",
       "6       0.125  \n",
       "7           0  \n",
       "8           0  \n",
       "9    0.166667  \n",
       "10  0.0652174  \n",
       "11  0.0652174  \n",
       "12          0  \n",
       "13          0  \n",
       "14      0.125  \n",
       "15          0  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #100, lr = 0.01, patience 2 +++, rms ver, rms05 ver\n",
    "# model_copy = model_dict.copy()\n",
    "# hist_copy = hist_dict.copy()\n",
    "# tmp_df = test_pred_df.copy()\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "      <th>rms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.287378</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.321318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.582811</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.0828107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.587976</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.0227588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0845234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.453548</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.00478577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.332341</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.0406745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.544543</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.169543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.41319</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0868104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434083</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.0659173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.746169</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.0795019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.624149</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.189366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.45127</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0164875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.512952</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.0129524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.510027</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.0100266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.530251</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.0947489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.212517</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.287483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.564185</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.216359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.56739</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.0992771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.708846</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.317542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583036</td>\n",
       "      <td>0.73913</td>\n",
       "      <td>0.73913</td>\n",
       "      <td>0.000297268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.401941</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.0147252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.502967</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.00296724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.596969</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.180302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.41666</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.125006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.525472</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0671387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.400999</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.00969421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.426227</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.115439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.380567</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.2861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.315825</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.205914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.0326918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.432563</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.109104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.273628</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.309706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2          rms\n",
       "0   2016   LG  0.608696  0.287378    0.583333    0.458333     0.321318\n",
       "1   2016   HH       0.5  0.582811         0.5    0.608696    0.0828107\n",
       "2   2016   NC  0.565217  0.587976         0.5    0.541667    0.0227588\n",
       "3   2016   HT  0.458333  0.542857         0.5    0.583333    0.0845234\n",
       "4   2016   SK  0.458333  0.453548    0.458333    0.458333   0.00478577\n",
       "5   2016   KT  0.291667  0.332341    0.333333       0.375    0.0406745\n",
       "6   2016   WO     0.375  0.544543       0.625    0.666667     0.169543\n",
       "7   2016   LT       0.5   0.41319       0.375         0.5    0.0868104\n",
       "8   2016   SS       0.5  0.434083         0.5    0.434783    0.0659173\n",
       "9   2016   OB  0.666667  0.746169    0.666667    0.458333    0.0795019\n",
       "10  2017   LG  0.434783  0.624149    0.304348    0.652174     0.189366\n",
       "11  2017   HH  0.434783   0.45127    0.541667    0.333333    0.0164875\n",
       "12  2017   NC       0.5  0.512952    0.458333    0.541667    0.0129524\n",
       "13  2017   HT       0.5  0.510027         0.5    0.782609    0.0100266\n",
       "14  2017   SK     0.625  0.530251    0.458333    0.416667    0.0947489\n",
       "15  2017   KT       0.5  0.212517    0.333333    0.166667     0.287483\n",
       "16  2017   WO  0.347826  0.564185         0.5    0.541667     0.216359\n",
       "17  2017   LT  0.666667   0.56739        0.75    0.590909    0.0992771\n",
       "18  2017   SS  0.391304  0.708846    0.333333    0.454545     0.317542\n",
       "19  2017   OB  0.583333  0.583036     0.73913     0.73913  0.000297268\n",
       "20  2018   LG  0.416667  0.401941    0.291667    0.478261    0.0147252\n",
       "21  2018   HH       0.5  0.502967    0.416667    0.583333   0.00296724\n",
       "22  2018   NC  0.416667  0.596969    0.521739    0.416667     0.180302\n",
       "23  2018   HT  0.541667   0.41666    0.541667       0.375     0.125006\n",
       "24  2018   SK  0.458333  0.525472         0.5    0.666667    0.0671387\n",
       "25  2018   KT  0.391304  0.400999    0.416667    0.545455   0.00969421\n",
       "26  2018   WO  0.541667  0.426227    0.666667    0.458333     0.115439\n",
       "27  2018   LT  0.666667  0.380567    0.458333    0.347826       0.2861\n",
       "28  2018   SS  0.521739  0.315825    0.565217    0.478261     0.205914\n",
       "29  2018   OB     0.625  0.592308       0.625       0.625    0.0326918\n",
       "30  2019   LG  0.541667  0.432563    0.541667         0.5     0.109104\n",
       "31  2019   HH  0.583333  0.273628       0.375        0.25     0.309706"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.01, patience 2 +++, rms ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.435018</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.534284</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.626261</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.544398</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.556326</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.371262</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.670688</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.443552</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.401195</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.483709</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.484057</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.458554</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2017</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.416243</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2017</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.579688</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.45835</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25805</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.517082</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.498034</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.44207</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.549855</td>\n",
       "      <td>0.73913</td>\n",
       "      <td>0.73913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.401864</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.589958</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.363228</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.458646</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.537073</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2018</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.457525</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.566216</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.428455</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.347826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.613215</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.59965</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.521697</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.261827</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.567362</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.442509</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.705648</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.593616</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.678757</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.389316</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0   2016   LG  0.608696  0.435018    0.583333    0.458333\n",
       "1   2016   HH       0.5  0.534284         0.5    0.608696\n",
       "2   2016   NC  0.565217  0.626261         0.5    0.541667\n",
       "3   2016   HT  0.458333  0.544398         0.5    0.583333\n",
       "4   2016   SK  0.458333  0.556326    0.458333    0.458333\n",
       "5   2016   KT  0.291667  0.371262    0.333333       0.375\n",
       "6   2016   WO     0.375  0.670688       0.625    0.666667\n",
       "7   2016   LT       0.5  0.443552       0.375         0.5\n",
       "8   2016   SS       0.5  0.401195         0.5    0.434783\n",
       "9   2016   OB  0.666667  0.483709    0.666667    0.458333\n",
       "10  2017   LG  0.434783  0.484057    0.304348    0.652174\n",
       "11  2017   HH  0.434783  0.458554    0.541667    0.333333\n",
       "12  2017   NC       0.5  0.416243    0.458333    0.541667\n",
       "13  2017   HT       0.5  0.579688         0.5    0.782609\n",
       "14  2017   SK     0.625   0.45835    0.458333    0.416667\n",
       "15  2017   KT       0.5   0.25805    0.333333    0.166667\n",
       "16  2017   WO  0.347826  0.517082         0.5    0.541667\n",
       "17  2017   LT  0.666667  0.498034        0.75    0.590909\n",
       "18  2017   SS  0.391304   0.44207    0.333333    0.454545\n",
       "19  2017   OB  0.583333  0.549855     0.73913     0.73913\n",
       "20  2018   LG  0.416667  0.401864    0.291667    0.478261\n",
       "21  2018   HH       0.5  0.589958    0.416667    0.583333\n",
       "22  2018   NC  0.416667  0.363228    0.521739    0.416667\n",
       "23  2018   HT  0.541667  0.458646    0.541667       0.375\n",
       "24  2018   SK  0.458333  0.537073         0.5    0.666667\n",
       "25  2018   KT  0.391304  0.457525    0.416667    0.545455\n",
       "26  2018   WO  0.541667  0.566216    0.666667    0.458333\n",
       "27  2018   LT  0.666667  0.428455    0.458333    0.347826\n",
       "28  2018   SS  0.521739  0.613215    0.565217    0.478261\n",
       "29  2018   OB     0.625   0.59965       0.625       0.625\n",
       "30  2019   LG  0.541667  0.521697    0.541667         0.5\n",
       "31  2019   HH  0.583333  0.261827       0.375        0.25\n",
       "32  2019   NC  0.565217  0.567362    0.541667    0.478261\n",
       "33  2019   HT       0.5  0.442509    0.478261    0.416667\n",
       "34  2019   SK     0.375  0.705648       0.625    0.708333\n",
       "35  2019   KT  0.541667  0.593616    0.521739    0.652174\n",
       "36  2019   WO  0.666667  0.678757    0.565217    0.708333\n",
       "37  2019   LT  0.208333  0.389316    0.391304    0.304348"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.01, patience 2 ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.288105</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.504434</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.539863</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.548684</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.418596</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.35347</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.663121</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.495439</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.383013</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.541724</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.475225</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0   2016   LG  0.608696  0.288105    0.583333    0.458333\n",
       "1   2016   HH       0.5  0.504434         0.5    0.608696\n",
       "2   2016   NC  0.565217  0.539863         0.5    0.541667\n",
       "3   2016   HT  0.458333  0.548684         0.5    0.583333\n",
       "4   2016   SK  0.458333  0.418596    0.458333    0.458333\n",
       "5   2016   KT  0.291667   0.35347    0.333333       0.375\n",
       "6   2016   WO     0.375  0.663121       0.625    0.666667\n",
       "7   2016   LT       0.5  0.495439       0.375         0.5\n",
       "8   2016   SS       0.5  0.383013         0.5    0.434783\n",
       "9   2016   OB  0.666667  0.541724    0.666667    0.458333\n",
       "10  2017   LG  0.434783    0.4588    0.304348    0.652174\n",
       "11  2017   HH  0.434783  0.475225    0.541667    0.333333"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.01, patience 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.467328</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.527652</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.52047</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.429098</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.4829</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.467328    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.527652         0.5    0.608696\n",
       "2  2016   NC  0.565217   0.52047         0.5    0.541667\n",
       "3  2016   HT  0.458333  0.429098         0.5    0.583333\n",
       "4  2016   SK  0.458333    0.4829    0.458333    0.458333"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.542802</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.545997</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.567216</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.577572</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.604854</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.42159</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.598227</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.386472</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.329622</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.542802    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.545997         0.5    0.608696\n",
       "2  2016   NC  0.565217  0.567216         0.5    0.541667\n",
       "3  2016   HT  0.458333  0.577572         0.5    0.583333\n",
       "4  2016   SK  0.458333  0.604854    0.458333    0.458333\n",
       "5  2016   KT  0.291667   0.42159    0.333333       0.375\n",
       "6  2016   WO     0.375  0.598227       0.625    0.666667\n",
       "7  2016   LT       0.5  0.386472       0.375         0.5\n",
       "8  2016   SS       0.5  0.329622         0.5    0.434783"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.478348</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.529292</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.478348    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.529292         0.5    0.608696"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.494715</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.630847</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.489612</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.593727</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.523688</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.243116</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.637864</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.400517</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.314818</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.494715    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.630847         0.5    0.608696\n",
       "2  2016   NC  0.565217  0.489612         0.5    0.541667\n",
       "3  2016   HT  0.458333  0.593727         0.5    0.583333\n",
       "4  2016   SK  0.458333  0.523688    0.458333    0.458333\n",
       "5  2016   KT  0.291667  0.243116    0.333333       0.375\n",
       "6  2016   WO     0.375  0.637864       0.625    0.666667\n",
       "7  2016   LT       0.5  0.400517       0.375         0.5\n",
       "8  2016   SS       0.5  0.314818         0.5    0.434783"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #100, lr=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.545502</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.607057</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.607852</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>SK</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.419407</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>KT</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.42172</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016</td>\n",
       "      <td>WO</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.52766</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016</td>\n",
       "      <td>LT</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.30456</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016</td>\n",
       "      <td>SS</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.501613</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016</td>\n",
       "      <td>OB</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.591436</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2017</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.441216</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2017</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.444414</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0   2016   LG  0.608696  0.545502    0.583333    0.458333\n",
       "1   2016   HH       0.5  0.636396         0.5    0.608696\n",
       "2   2016   NC  0.565217  0.607057         0.5    0.541667\n",
       "3   2016   HT  0.458333  0.607852         0.5    0.583333\n",
       "4   2016   SK  0.458333  0.419407    0.458333    0.458333\n",
       "5   2016   KT  0.291667   0.42172    0.333333       0.375\n",
       "6   2016   WO     0.375   0.52766       0.625    0.666667\n",
       "7   2016   LT       0.5   0.30456       0.375         0.5\n",
       "8   2016   SS       0.5  0.501613         0.5    0.434783\n",
       "9   2016   OB  0.666667  0.591436    0.666667    0.458333\n",
       "10  2017   LG  0.434783  0.441216    0.304348    0.652174\n",
       "11  2017   HH  0.434783  0.444414    0.541667    0.333333"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #200, lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.392572</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.548009</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.392572    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.548009         0.5    0.608696"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #200, lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>T_ID</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>shift_PCT_1</th>\n",
       "      <th>shift_PCT_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>LG</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.513151</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.458333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>HH</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.573316</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>NC</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>HT</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.470724</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR T_ID         y    y_pred shift_PCT_1 shift_PCT_2\n",
       "0  2016   LG  0.608696  0.513151    0.583333    0.458333\n",
       "1  2016   HH       0.5  0.573316         0.5    0.608696\n",
       "2  2016   NC  0.565217  0.635897         0.5    0.541667\n",
       "3  2016   HT  0.458333  0.470724         0.5    0.583333"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_df #200, lr=0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist1.history['loss'], 'y', label='train loss')\n",
    "\n",
    "acc_ax.plot(hist1.history['mae'], 'b', label='train mae')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('mae')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
